{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fifi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "file_path=\"img_decoding.pdf\"\n",
    "\n",
    "# Extracting Text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF and split it into sentences\n",
    "text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mbeddings obtained from the image, ii) an MEG\n",
      "module trained end-to-end and iii) a pretrained image generator. Our results are\n",
      "threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval\n",
      "over classic linear decoders. Second, late brain responses to images are best de-\n",
      "coded with DINOv2, a recent foundational image model. Third, image retrievals\n",
      "and generations both suggest that MEG signals primarily contain high-level visual\n",
      "features, whereas the same approach applied to 7T fMRI also recovers low-level\n",
      "features. Overall, these results provide an important step towards the decoding\n",
      "– in real time – of the visual processes continuously unfolding within the human\n",
      "brain.\n",
      "1 I NTRODUCTION\n",
      "Automating the discovery of brain representations. Understanding how the human brain rep-\n",
      "resents the world is arguably one of the most profound scientific challenges. This quest, which\n",
      "originally consisted of searching, one by one, for the specific features that trigger each neuron, ( e.g.\n",
      "Hubel & Wiesel (1962); O’Keefe & Nadel (1979); Kanwisher et al. (1997)), is now being automated\n",
      "by Machine Learning (ML) in two mains ways. First, as a signal processing tool, ML algorithms are\n",
      "trained to extract informative patterns of brain activity in a data-driven manner. For example, Kami-\n",
      "tani & Tong (2005) trained a support vector machine to classify the orientations of visual gratings\n",
      "from functional Magnetic Resonance Imaging (fMRI). Since then, deep learning has been increas-\n",
      "ingly used to discover such brain activity patterns (Roy et al., 2019; Thomas et al., 2022; Jayaram\n",
      "& Barachant, 2018; D ´efossez et al., 2022; Scotti et al., 2023). Second, ML algorithms are used as\n",
      "functional models of the brain. For example, Yamins et al. (2014) have shown that the embedding\n",
      "of natural images in pretrained deep nets linearly account for the neuronal responses to these images\n",
      "in the cortex. Since, pretrained deep learning models have been shown to account for a wide variety\n",
      "of stimuli including text, spee\n"
     ]
    }
   ],
   "source": [
    "sample = text[1015:3037]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRAIN DECODING :TOWARD REAL -TIME\\nRECONSTRUCTION OF VISUAL PERCEPTION\\nYohann Benchetrit1,∗, Hubert Banville1,∗, Jean-R ´emi King1,2\\n1FAIR, Meta,2Laboratoire des Syst `emes Perceptifs, ´Ecole Normale Sup ´erieure, PSL University\\n{ybenchetrit,hubertjb,jeanremi }@meta.com\\nABSTRACT\\nIn the past five years, the use of generative and foundational AI systems has\\ngreatly improved the decoding of brain activity.',\n",
       " 'Visual perception, in particular,\\ncan now be decoded from functional Magnetic Resonance Imaging (fMRI) with\\nremarkable fidelity.',\n",
       " 'This neuroimaging technique, however, suffers from a lim-\\nited temporal resolution ( ≈0.5 Hz) and thus fundamentally constrains its real-time\\nusage.',\n",
       " 'Here, we propose an alternative approach based on magnetoencephalog-\\nraphy (MEG), a neuroimaging device capable of measuring brain activity with\\nhigh temporal resolution ( ≈5,000 Hz).',\n",
       " 'For this, we develop an MEG decoding\\nmodel trained with both contrastive and regression objectives and consisting of\\nthree modules: i) pretrained embeddings obtained from the image, ii) an MEG\\nmodule trained end-to-end and iii) a pretrained image generator.',\n",
       " 'Our results are\\nthreefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval\\nover classic linear decoders.',\n",
       " 'Second, late brain responses to images are best de-\\ncoded with DINOv2, a recent foundational image model.',\n",
       " 'Third, image retrievals\\nand generations both suggest that MEG signals primarily contain high-level visual\\nfeatures, whereas the same approach applied to 7T fMRI also recovers low-level\\nfeatures.',\n",
       " 'Overall, these results provide an important step towards the decoding\\n– in real time – of the visual processes continuously unfolding within the human\\nbrain.',\n",
       " '1 I NTRODUCTION\\nAutomating the discovery of brain representations.',\n",
       " 'Understanding how the human brain rep-\\nresents the world is arguably one of the most profound scientific challenges.',\n",
       " 'This quest, which\\noriginally consisted of searching, one by one, for the specific features that trigger each neuron, ( e.g.',\n",
       " 'Hubel & Wiesel (1962); O’Keefe & Nadel (1979); Kanwisher et al.',\n",
       " '(1997)), is now being automated\\nby Machine Learning (ML) in two mains ways.',\n",
       " 'First, as a signal processing tool, ML algorithms are\\ntrained to extract informative patterns of brain activity in a data-driven manner.',\n",
       " 'For example, Kami-\\ntani & Tong (2005) trained a support vector machine to classify the orientations of visual gratings\\nfrom functional Magnetic Resonance Imaging (fMRI).',\n",
       " 'Since then, deep learning has been increas-\\ningly used to discover such brain activity patterns (Roy et al., 2019; Thomas et al., 2022; Jayaram\\n& Barachant, 2018; D ´efossez et al., 2022; Scotti et al., 2023).',\n",
       " 'Second, ML algorithms are used as\\nfunctional models of the brain.',\n",
       " 'For example, Yamins et al.',\n",
       " '(2014) have shown that the embedding\\nof natural images in pretrained deep nets linearly account for the neuronal responses to these images\\nin the cortex.',\n",
       " 'Since, pretrained deep learning models have been shown to account for a wide variety\\nof stimuli including text, speech, navigation, and motor movement (Banino et al., 2018; Schrimpf\\net al., 2020; Hausmann et al., 2021; Mehrer et al., 2021; Caucheteux et al., 2023).',\n",
       " 'Generating images from brain activity.',\n",
       " 'This observed representational alignment between brain\\nactivity and deep learning models creates a new opportunity: Decoding of visual stimuli need not\\nbe restricted to a limited set of classes, but can now leverage pretrained representations to condi-\\ntion subsequent generative AI models.',\n",
       " 'While the resulting image may be partly “hallucinated”,\\ninterpreting images can be much simpler than interpreting latent features.',\n",
       " 'Following a long series\\n∗Equal contribution.',\n",
       " '1 of generative approaches (Nishimoto et al., 2011; Kamitani & Tong, 2005; VanRullen & Reddy,\\n2019; Seeliger et al., 2018), diffusion techniques have, in this regard, significantly improved the\\ngeneration of images from functional Magnetic Resonance Imaging (fMRI).',\n",
       " 'The resulting pipeline\\ntypically consists of three main modules: (1) a set of pretrained embeddings obtained from the im-\\nage onto which (2) fMRI activity can be linearly mapped and (3) ultimately used to condition a\\npretrained image-generation model (Ozcelik & VanRullen, 2023; Mai & Zhang, 2023; Zeng et al.,\\n2023; Ferrante et al., 2022).',\n",
       " 'These recent fMRI studies primarily differ in the type of pretrained\\nimage-generation model that they use.',\n",
       " 'The challenge of real-time decoding.',\n",
       " 'This generative decoding approach has been mainly applied\\nto fMRI.',\n",
       " 'However, the temporal resolution of fMRI is limited by the time scale of blood flow and\\ntypically leads to one snapshot of brain activity every two seconds – a time scale that challenges its\\nclinical usage, e.g.',\n",
       " 'for patients who require a brain-computer-interface (Willett et al., 2023; Moses\\net al., 2021; Metzger et al., 2023; D ´efossez et al., 2022).',\n",
       " 'On the contrary, magnetoencephalography\\n(MEG) can measure brain activity at a much higher temporal resolution ( ≈5,000 Hz) by recording\\nthe fluctuation of magnetic fields elicited by the post-synaptic potentials of pyramidal neurons.',\n",
       " 'This\\nhigher temporal resolution comes at cost, however: the spatial resolution of MEG is limited to ≈300\\nsensors, whereas fMRI measures ≈100,000 voxels.',\n",
       " 'In sum, fMRI intrinsically limits our ability to\\n(1) track the dynamics of neuronal activity, (2) decode dynamic stimuli (speech, videos etc) and\\n(3) apply these tools to real-time use cases.',\n",
       " 'Conversely, it is unknown whether temporally-resolved\\nneuroimaging systems like MEG are sufficiently precise to generate natural images in real-time.',\n",
       " 'Our approach.',\n",
       " 'Combining previous work on speech retrieval from MEG (D ´efossez et al., 2022)\\nand on image generation from fMRI (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023),\\nwe here develop a three-module pipeline trained to (1) align MEG activity onto pretrained visual\\nembeddings and (2) generate images from a stream of MEG signals (Fig.',\n",
       " '1).',\n",
       " 'Figure 1: ( A) Approach.',\n",
       " 'Locks indicate pretrained models.',\n",
       " '( B) Processing schemes.',\n",
       " 'Unlike image\\ngeneration, image retrieval can be done in the aligned latent space, but requires the true image in the\\nretrieval set.',\n",
       " 'Our systematic benchmark provides two main contributions: our MEG decoder leads to (1) high-\\nperforming image retrieval and image generation, (2) new means to interpret the unfolding of visual\\nprocessing in the brain.',\n",
       " 'This demonstrates the capacity of our approach to truly generalize to new\\nvisual concepts, paving the way to “free-form” visual decoding.',\n",
       " 'Overall, our findings outline a\\npromising avenue for real-time decoding of visual representations in the lab and in the clinic.',\n",
       " '2 2 M ETHODS\\n2.1 P ROBLEM STATEMENT\\nWe aim to decode images from multivariate time series of brain activity recorded with MEG as\\nhealthy participants watched a sequence of natural images.',\n",
       " 'Let Xi∈RC×Tbe the MEG time\\nwindow collected as an image Iiwas presented to the participant, where Cis the number of MEG\\nchannels, Tis the number of time points in the MEG window and i∈[ [1, N] ].',\n",
       " 'Letzi∈RFbe the\\nlatent representation of Ii, with Fthe number of features, obtained by embedding the image using\\na pretrained image model (Section 2.4).',\n",
       " 'As described in more detail below, our decoding approach\\nrelies on training a brain module fθ:RC×T→RFto maximally retrieve or predict Iithrough zi,\\ngiven Xi.',\n",
       " '2.2 T RAINING OBJECTIVES\\nWe use different training objectives for the different parts of our proposed pipeline.',\n",
       " 'First, in the case\\nof retrieval, we aim to pick the right image Ii(i.e., the one corresponding to Xi) out of a bank of\\ncandidate images.',\n",
       " 'To do so, we train fθusing the CLIP loss (Radford et al., 2021) on batches of size\\nB with exactly one positive example:\\nLCLIP (θ) =−1\\nBBX\\ni=1 \\nlogexp(s(ˆzi,zi)/τ)PB\\nj=1exp(s(ˆzi,zj)/τ)+ logexp(s(ˆzi,zi)/τ)PB\\nk=1exp(s(ˆzk,zi)/τ)!',\n",
       " '(1)\\nwhere sis the cosine similarity, ziandˆzi=fθ(Xi)are the latent representation and the correspond-\\ning MEG-based prediction, respectively, and τis a learned temperature parameter.',\n",
       " 'Next, to go beyond retrieval and instead generate images, we train fθto directly predict the latent\\nrepresentations zsuch that we can use them to condition generative image models.',\n",
       " 'This is done\\nusing a standard mean squared error (MSE) loss:\\nLMSE(θ) =1\\nNFNX\\ni=1∥zi−ˆzi∥2\\n2 (2)\\nFinally, we combine the CLIP and MSE losses using a convex combination with tuned weight to\\ntrain models that benefit from both training objectives:\\nLCombined =λLCLIP + (1−λ)LMSE (3)\\n2.3 B RAIN MODULE\\nWe adapt the dilated residual ConvNet architecture of D ´efossez et al.',\n",
       " '(2022), denoted as fθ, to\\nlearn the projection from an MEG window Xi∈RC×Tto a latent image representation zi∈RF.',\n",
       " 'The original model’s output ˆYbackbone ∈RF′×Tmaintains the temporal dimension of the network\\nthrough its residual blocks.',\n",
       " 'However, here we regress a single latent per input instead of a sequence\\nofTlatents like in D ´efossez et al.',\n",
       " '(2022).',\n",
       " 'Consequently, we add a temporal aggregation layer\\nto reduce the temporal dimension of ˆYbackbone to obtain ˆyagg∈RF′.',\n",
       " 'We experiment with three\\ntypes of aggregations: global average pooling, a learned affine projection, and an attention layer.',\n",
       " 'Finally, we add two MLP heads1,i.e., one for each term in LCombined , to project from F′to the F\\ndimensions of the target latent.',\n",
       " 'We run a hyperparameter search to identify an appropriate configuration of preprocessing, brain\\nmodule architecture, optimizer and loss hyperparameters for the retrieval task (see Appendix A.2).',\n",
       " 'The final architecture configuration for retrieval is described in Table S2 and contains e.g.',\n",
       " '6.4M\\ntrainable parameters for F= 768 .',\n",
       " '1A head consists of repeated LayerNorm-GELU-Linear blocks.',\n",
       " '3 For image generation experiments, the output of the MSE head is further postprocessed as in Ozcelik\\n& VanRullen (2023), i.e., we z-score normalize each feature across predictions, and then apply the\\ninverse z-score transform fitted on the training set (defined by the mean and standard deviation of\\neach feature dimension on the target embeddings).',\n",
       " 'We select λinLCombined by sweeping over\\n{0.0,0.25,0.5,0.75,1.0}and pick the model whose top-5 accuracy is the highest on the large test\\nset.',\n",
       " 'Of note, when training models to generate CLIP and AutoKL latents, we simplify the task of\\nthe CLIP head by reducing the dimensionality of its target: we use the CLS token for CLIP-Vision\\n(FMSE = 768 ), the ”mean” token for CLIP-Text ( FMSE = 768 ), and the channel-average for\\nAutoKL latents ( FMSE = 4096 ), respectively.',\n",
       " '2.4 I MAGE MODULES\\nWe study the functional alignment between brain activity and a variety of (output) embeddings\\nobtained from deep neural networks trained in three different representation learning paradigms,\\nspanning a wide range of dimensionalities: supervised learning ( e.g.',\n",
       " 'VGG-19), image-text align-\\nment (CLIP), and variational autoencoders.',\n",
       " 'When using vision transformers, we further include two\\nadditional embeddings of smaller dimensionality: the average of all output embeddings across to-\\nkens (mean), and the output embedding of the class-token (CLS).',\n",
       " 'For comparison, we also evaluate\\nour approach on human-engineered features obtained without deep learning.',\n",
       " 'The list of embeddings\\nis provided in Appendix A.4.',\n",
       " 'For clarity, we focus our experiments on a representative subset.',\n",
       " '2.5 G ENERATION MODULE\\nTo fairly compare our work to the results obtained with fMRI results, we follow the approach of\\nOzcelik & VanRullen (2023) and use a model trained to generate images from pretrained embed-\\ndings.',\n",
       " 'Specifically, we use a latent diffusion model conditioned on three embeddings: CLIP-Vision\\n(257tokens ×768), CLIP-Text ( 77tokens ×768), and a variational autoencoder latent (AutoKL;\\n(4×64×64).',\n",
       " 'Following Ozcelik & VanRullen (2023), we apply diffusion with 50 DDIM steps, a\\nguidance of 7.5, a strength of 0.75 with respect to the image-to-image pipeline, and a mixing of 0.4.',\n",
       " '2.6 T RAINING AND COMPUTATIONAL CONSIDERATIONS\\nCross-participant models are trained on a set of ≈63,000 examples using the Adam optimizer\\n(Kingma & Ba, 2014) with learning rate of 3×10−4and a batch size of 128.',\n",
       " 'We use early stopping\\non a validation set of ≈15,800examples randomly sampled from the original training set, with a\\npatience of 10, and evaluate the performance of the model on a held-out test set (see below).',\n",
       " 'Models\\nare trained on a single V olta GPU with 32 GB of memory.',\n",
       " 'We train each model three times using\\nthree different random seeds for the weight initialization of the brain module.',\n",
       " '2.7 E VALUATION\\nRetrieval metrics.',\n",
       " 'We first evaluate decoding performance using retrieval metrics.',\n",
       " 'For a known\\ntest set, we are interested in the probability of identifying the correct image given the model predic-\\ntions.',\n",
       " 'Retrieval metrics have the advantage of sharing the same scale regardless of the dimensional-\\nity of the MEG (like encoding metrics), the dimensionality of the image embedding (like regression\\nmetrics).',\n",
       " 'We evaluate retrieval using either the relative median rank (which does not depend on the\\nsize of the retrieval set), defined as the rank of a prediction divided by the size of the retrieval set, or\\nthetop-5 accuracy (which is more common in the literature).',\n",
       " 'Generation metrics.',\n",
       " 'Decoding performance is often measured qualitatively as well as quantita-\\ntively using a variety of metrics reflecting the reconstruction fidelity both in terms of perception and\\nsemantics.',\n",
       " 'For fair comparison with fMRI generations, we provide the same metrics as Ozcelik &\\nVanRullen (2023), computed between seen and generated images: PixCorr (the pixel-wise correla-\\ntion between the true and generated images), SSIM (Structural Similarity Index Metric), and SwA V\\n(the correlation with respect to SwA V-ResNet50 output).',\n",
       " 'On the other hand, AlexNet(2/5), Incep-\\ntion, and CLIP are the respective 2-way comparison scores of layers 2/5 of AlexNet, the pooled last\\nlayer of Inception and the output layer of CLIP.',\n",
       " 'For the NSD dataset, these metrics are reported for\\nparticipant 1 only (see Appendix A.5).',\n",
       " '4 To avoid non-representative cherry-picking, we sort all generations on the test set according to the\\nsum of (minus) SwA V and SSIM.',\n",
       " 'We then split the data into 15 blocks and pick 4 images from the\\nbest, middle and worst blocks with respect to the summed metric.',\n",
       " 'Real-time and average metrics.',\n",
       " 'It is common in fMRI to decode brain activity from preprocessed\\nvalues estimated with a General Linear Model.',\n",
       " 'These “beta values” are estimates of brain responses\\nto individual images, computed across multiple repetitions of such images.',\n",
       " 'To provide a fair as-\\nsessment of possible MEG decoding performance, we thus leverage repeated image presentations\\navailable in the datasets (see below) by averaging predictions before evaluating metrics.',\n",
       " '2.8 D ATASET\\nWe test our approach on the “THINGS-MEG” dataset (Hebart et al., 2023).',\n",
       " 'Four participants (2\\nfemales, 2 males; mean age of 23.25 years), underwent 12 MEG sessions during which they were\\npresented with a set of 22,448 unique images selected from the THINGS database (Hebart et al.,\\n2019), covering 1,854 categories.',\n",
       " 'Of those, only a subset of 200 images (each one of a different cat-\\negory) was shown multiple times to the participants.',\n",
       " 'The images were displayed for 500 ms each,\\nwith a variable fixation period of 1000±200ms between presentations.',\n",
       " 'The THINGS dataset ad-\\nditionally contains 3,659 images that were not shown to the participants and that we use to augment\\nthe size of our retrieval set and emphasize the robustness of our method.',\n",
       " 'MEG Preprocessing.',\n",
       " 'We use a minimal MEG data-preprocessing pipeline as in D ´efossez et al.',\n",
       " '(2022).',\n",
       " 'Raw data from the 272 MEG radial gradiometer channels is downsampled from 1,200 Hz to\\n120 Hz before being centered and clipped channel-wise above +/- 5 standard errors.',\n",
       " 'The continuous\\nMEG data is then epoched from -500 ms to 1,000 ms relative to stimulus onset.',\n",
       " 'Finally, baseline-\\ncorrection is performed by subtracting the mean signal value observed between the start of an epoch\\nand the stimulus onset for each channel.',\n",
       " 'Splits.',\n",
       " 'The original split of Hebart et al.',\n",
       " '(2023) consists of 22,248 uniquely presented images,\\nand 200 test images repeated 12 times each for each participant ( i.e., 2,400 trials per participant).',\n",
       " 'The use of this data split presents a challenge, however, as the test set contains only one image\\nper category, and these categories are also seen in the training set.',\n",
       " 'This means evaluating retrieval\\nperformance on this test set does not measure the capacity of the model to (1) extrapolate to new\\nunseen categories of images and (2) recover a particular image within a set of multiple images of\\nthe same category, but rather only to “categorize” it.',\n",
       " 'Consequently, we propose two modifications\\nof the original split.',\n",
       " 'First, we remove from the training set any image whose category appears in the\\noriginal test set.',\n",
       " 'This “adapted training set” removes any categorical leakage across the train/test\\nsplit and makes it possible to assess the capacity of the model to decode images of unseen image\\ncategories ( i.e., a “zero-shot” setting).',\n",
       " 'Second, we propose a new “large test set” that is built using\\nthe images removed from the training set.',\n",
       " 'This new test set effectively allows evaluating retrieval\\nperformance of images within images of the same category2.',\n",
       " 'We report results on both the original\\n(“small”) and the “large” test sets to enable comparisons with the original settings of Ozcelik &\\nVanRullen (2023).',\n",
       " 'Finally, we also compare our results to the performance obtained by a similar\\npipeline but trained on fMRI data using the NSD dataset (Allen et al., 2022) (see Appendix A.5).',\n",
       " '3 R ESULTS\\nML as an effective model of the brain.',\n",
       " 'Which representations of natural images are likely to\\nmaximize decoding performance?',\n",
       " 'To answer this question, we compare the retrieval performance\\nobtained by linear Ridge regression models trained to predict one of 16 different latent visual rep-\\nresentations given the flattened MEG response Xito each image Ii(Table S1).',\n",
       " 'While all image\\nembeddings lead to above-chance retrieval, supervised and text/image alignment models ( e.g.',\n",
       " 'VGG,\\nCLIP) yield the highest retrieval scores.',\n",
       " '2We leave out images of the original test set from this new large test set, as keeping them would create a\\ndiscrepancy between the number of MEG repetitions for training images and test images.',\n",
       " '5 ML as an effective toolto learn brain responses.',\n",
       " 'We then compare these linear baselines to\\na deep ConvNet architecture (D ´efossez et al., 2022) trained on the same task3,i.e., to retrieve the\\nmatching image given an MEG window.',\n",
       " 'Using a deep model leads to a 7X improvement over the lin-\\near baselines (Fig.',\n",
       " '2).',\n",
       " 'Multiple types of image embeddings lead to good retrieval performance, with\\nVGG-19 (supervised learning), CLIP-Vision (text/image alignment) and DINOv2 (self-supervised\\nlearning) yielding top-5 accuracies of 70.33±2.80%,68.66±2.84%,68.00±2.86%, respectively\\n(where the standard error of the mean is computed across the averaged image-wise metrics).',\n",
       " 'Similar\\nconclusions, although with lower performance, can be drawn from our “large” test set setting, where\\ndecoding cannot rely solely on the image category but also requires discriminating between multiple\\nimages of the same category.',\n",
       " 'Representative retrieval examples are shown in Appendix A.3.',\n",
       " 'Figure 2: Image retrieval performance obtained from a trained deep ConvNet.',\n",
       " 'The original “small”\\ntest set (Hebart et al., 2023) comprises 200 distinct images, each belonging to a different category.',\n",
       " 'In contrast, our proposed “large” test set comprises 12 images from each of those 200 categories,\\nyielding a total of 2,400 images.',\n",
       " 'Chance-level is 2.5% top-5 accuracy for the small test set and\\n0.21% for the large test set.',\n",
       " 'The best latent representations yield accuracies around 70% and 13%\\nfor the small and large test sets, respectively.',\n",
       " 'Temporally-resolved image retrieval.',\n",
       " 'The above results are obtained from the full time window\\n(-500 ms to 1,000 ms relative to stimulus onset).',\n",
       " 'To further investigate the possibility of decoding\\nvisual representations as they unfold in the brain, we repeat this analysis on 250 ms-long sliding win-\\ndows (Fig.',\n",
       " '3).',\n",
       " 'For clarity, we focus on a subset of representative image embeddings.',\n",
       " 'As expected,\\nall models yield chance-level performance before the image presentation.',\n",
       " 'For all models, a first\\nclear peak can then be observed on the 0 to 250-ms window, followed by a second peak, after the\\nimage offset, which then quickly goes back to chance-level.',\n",
       " 'Interestingly, the recent self-supervised\\nmodel DINOv2 yields particularly good retrieval performance after the image offset.',\n",
       " 'To get a better sense of what the above decoding metrics mean, we present the top-1 retrieved images\\nfrom an augmented retrieval set built by concatenating the “large” test set with an additional set of\\n3,659 images that were not seen by the participants (Fig.',\n",
       " '4).',\n",
       " 'Overall, the retrieved images tend to come from the correct category, such as “speaker” or “brocoli”,\\nmostly during the first few sub-windows ( t≤1s).',\n",
       " 'However, these retrieved images do not appear\\nto share obvious low-level features to the images seen by the participants.',\n",
       " '3We use λ= 1inLCombined as we are solely concerned with the retrieval part of the pipeline here.',\n",
       " '6 Figure 3: Retrieval performance of models trained on 250 ms sliding windows for different image\\nembeddings.',\n",
       " 'The shaded gray area indicates the 0.5-s interval during which the image was presented\\nto the participants.',\n",
       " 'Accuracy generally peaked right after the image onset and offset.',\n",
       " 'Figure 4: Representative examples of dynamic retrievals using CLIP-Vision (CLS) and models\\ntrained on 250-ms sliding windows (Image onset: t= 0, retrieval set: N= 6,059from 1,196\\ncategories).',\n",
       " 'The groups of three stacked rows represent best, average and worst retrievals, obtained\\nby sampling examples from the <10%, 45-55% and >90% percentile groups based on top-5 accu-\\nracy.',\n",
       " 'Overall, and while further analyses of these results remain necessary, it seems that (1) our decoding\\nleverages the brain responses related to both the onset and the offset of the image and (2) category-\\nlevel information dominates these visual representations as early as 250 ms.\\n7 Table 1: Quantitative evaluation of reconstruction quality from MEG data on THINGS-MEG (com-\\npared to fMRI data on NSD (Allen et al., 2022) using a cross-validated Ridge regression).',\n",
       " 'We re-\\nport PixCorr, SSIM, AlexNet(2), AlexNet(5), Inception, SwA V and CLIP (the side-arrow indicates\\nwhether better scores are higher or lower).',\n",
       " 'In particular, this shows that fMRI betas as provided in\\nNSD are significantly easier to decode than MEG signals from THINGS-MEG.',\n",
       " 'Low-level High-level\\nDataset PixCorr ↑SSIM↑AlexNet(2) ↑AlexNet(5) ↑Inception ↑CLIP↑SwA V ↓\\nNSD (fMRI) 0.305 0.366 0.962 0.977 0.910 0.917 0.410\\nTHINGS-MEG\\n(per-trial average)0.079 0.329 0.718 0.823 0.674 0.765 0.595\\nTHINGS-MEG\\n(per-subject average)0.088 0.333 0.747 0.855 0.712 0.804 0.576\\nTHINGS-MEG\\n(no average)0.069 0.308 0.668 0.733 0.613 0.668 0.636\\nGenerating images from MEG.',\n",
       " 'While framing decoding as a retrieval task yields promising\\nresults, it requires the true image to be in the retrieval set – a well-posed problem which presents\\nlimited use-cases in practice.',\n",
       " 'To address this issue, we trained three distinct brain modules to predict\\nthe three embeddings that we use (see Section 2.5) to generate images (Fig.',\n",
       " '5).',\n",
       " 'As confirmed by the\\nevaluation metrics of Table 1, the generated images look relatively good, with multiple generated\\nimages sharing the correct ground-truth category.',\n",
       " 'However, they appear to contain limited low-level\\ninformation about the true image.',\n",
       " 'Figure 5: Examples of generated images conditioned on MEG-based latent predictions.',\n",
       " 'The groups\\nof three stacked rows represent best, average and worst generations, as evaluated by the sum of\\n(minus) SwA V and SSIM.',\n",
       " 'The application of a very similar pipeline on an analogous fMRI dataset (Allen et al., 2022; Ozcelik\\n& VanRullen, 2023) – using a simple Ridge regression – shows image reconstructions that share\\nboth high-level and low-level features with the true image Fig.',\n",
       " 'S3).',\n",
       " 'Together, these results suggest\\nthat it is not the reconstruction pipeline which fails to reconstruct low-level features, but rather the\\nMEG signals which contain little information at that level.',\n",
       " '8 4 D ISCUSSION\\nRelated work.',\n",
       " 'The present study shares several elements with previous MEG and electroen-\\ncephalography (EEG) studies designed not to maximize decoding performance but to understand\\nthe cascade of visual processes in the brain.',\n",
       " 'In particular, previous studies have trained linear mod-\\nels to either (1) classify a small set of images from brain activity (Grootswagers et al., 2019; King\\n& Wyart, 2021), (2) predict brain activity from the latent representations of the images (Cichy et al.,\\n2017) or (3) quantify the similarity between these two modalities with representational similarity\\nanalysis (RSA) (Cichy et al., 2017; Bankson et al., 2018; Grootswagers et al., 2019; Gifford et al.,\\n2022).',\n",
       " 'While these studies also make use of image embeddings, their linear decoders are limited to\\nclassifying a small set of object classes, or to distinguishing pairs of images.',\n",
       " 'In addition, several deep neural networks have been introduced to maximize the classification of\\nspeech (D ´efossez et al., 2022), mental load (Jiao et al., 2018) and images (Palazzo et al., 2020;\\nMcCartney et al., 2022; Bagchi & Bathula, 2022) from EEG recordings.',\n",
       " 'In particular, Palazzo et al.',\n",
       " '(2020) introduced a deep convolutional neural network to classify natural images from EEG signals.',\n",
       " 'However, the experimental protocol consisted of presenting all of the images of the same class within\\na single continuous block, which risks allowing the decoder to rely on autocorrelated noise, rather\\nthan informative brain activity patterns (Li et al., 2020).',\n",
       " 'In any case, these EEG studies focus on the\\ncategorization of a relatively small number of images classes.',\n",
       " 'In sum, there is, to our knowledge, no MEG decoding study that learns end-to-end to reliably gen-\\nerate an open set of images.',\n",
       " 'Impact.',\n",
       " 'The present work has both fundamental and practical impacts.',\n",
       " 'First, the ability to decode\\ncomplex perceptual representations as a function of time promises to greatly facilitate our under-\\nstanding of the processes at stake during visual processing in the brain.',\n",
       " 'There is considerable work\\ninspecting the nature and the timing of the representations built along the visual system.',\n",
       " 'However,\\nthese results can be challenging to interpret, especially for high-level features.',\n",
       " 'Generative decoding,\\non the contrary, provides concrete and, thus, interpretable predictions.',\n",
       " 'Second, the most obvious\\nuse-case of brain decoding technology is to assist patients whose brain lesions challenge communi-\\ncation.',\n",
       " 'This use-case, however, requires real-time decoding, and thus limit the use of neuroimaging\\nmodalities with low temporal resolution such as fMRI.',\n",
       " 'The present effort thus paves the way to\\nachieve this long-awaited goal.',\n",
       " 'Limitations.',\n",
       " 'Our analyses highlight three main limitations to the decoding of images from MEG\\nsignals.',\n",
       " 'First, the decoding of high-level semantic features prevails over low-level features: in\\nparticular, the generated images preserve semantics ( e.g.',\n",
       " 'object categories) much better than low-\\nlevel features ( e.g.',\n",
       " 'contours, shading).',\n",
       " 'This phenomenon is difficult to attribute to our pipeline:\\nindeed, applying a similar procedure to 7T fMRI recordings achieves reasonably high reconstruction\\nof low-level features (Fig.',\n",
       " 'S3).',\n",
       " 'Rather, this result resonates with the fact that the spatial resolution\\nof MEG ( ≈cm) is much lower than 7T fMRI’s ( ≈mm).',\n",
       " 'Second, the present approach directly\\ndepends on the pretraining of several models, and only learns end-to-end to align the MEG signals\\nto these pretrained embeddings.',\n",
       " 'Our results show that this approach leads to better performance than\\nclassical computer vision features such as color histograms, fast-Fourier transforms and histogram\\nof oriented gradients (HOG).',\n",
       " 'This is consistent with a recent MEG study by D ´efossez et al.',\n",
       " '(2022)\\nwhich showed, in the context of speech decoding, that pretrained embeddings outperformed a fully\\nend-to-end approach.',\n",
       " 'Nevertheless, it remains to be tested whether (1) fine-tuning the image and\\ngeneration modules and (2) combining the different types of visual features could improve decoding\\nperformance.',\n",
       " 'Ethical implications.',\n",
       " 'While the decoding of brain activity promises to help a variety of brain-\\nlesioned patients (Metzger et al., 2023; Moses et al., 2021; D ´efossez et al., 2022; Liu et al., 2023;\\nWillett et al., 2023), the rapid advances of this technology raise several ethical considerations, and\\nmost notably, the necessity to preserve mental privacy.',\n",
       " 'Several empirical findings are relevant to this\\nissue.',\n",
       " 'Firstly, the decoding performance obtained with non-invasive recordings is only high for per-\\nceptual tasks.',\n",
       " 'By contrast, decoding accuracy considerably diminishes when individuals are tasked\\nto imagine representations (Horikawa & Kamitani, 2017; Tang et al., 2023).',\n",
       " 'Second, decoding per-\\nformance seems to be severely compromised when participants are engaged in disruptive tasks, such\\n9 as counting backward (Tang et al., 2023).',\n",
       " 'In other words, the subjects’ consent is not only a legal but\\nalso and primarily a technical requirement for brain decoding.',\n",
       " 'To delve into these issues effectively,\\nwe endorse the open and peer-reviewed research standards.',\n",
       " 'Conclusion.',\n",
       " 'Overall, these results provide an important step towards the decoding of the visual\\nprocesses continuously unfolding in the human brain.',\n",
       " 'REFERENCES\\nEmily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle,\\nMatthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al.',\n",
       " 'A massive 7T fMRI dataset to\\nbridge cognitive neuroscience and artificial intelligence.',\n",
       " 'Nature neuroscience , 25(1):116–126,\\n2022.',\n",
       " 'Subhranil Bagchi and Deepti R Bathula.',\n",
       " 'EEG-ConvTransformer for single-trial EEG-based visual\\nstimulus classification.',\n",
       " 'Pattern Recognition , 129:108757, 2022.',\n",
       " 'Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,\\nAlexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al.',\n",
       " 'Vector-based\\nnavigation using grid-like representations in artificial agents.',\n",
       " 'Nature , 557(7705):429–433, 2018.',\n",
       " 'B.B.',\n",
       " 'Bankson, M.N.',\n",
       " 'Hebart, I.I.A.',\n",
       " 'Groen, and C.I.',\n",
       " 'Baker.',\n",
       " 'The temporal evolution of conceptual\\nobject representations revealed through models of behavior, semantics and deep neural networks.',\n",
       " 'NeuroImage , 178:172–182, 2018.',\n",
       " 'ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.',\n",
       " '2018.05.037.',\n",
       " 'URL https://www.sciencedirect.com/science/article/pii/\\nS1053811918304440 .',\n",
       " 'G. Bradski.',\n",
       " 'The OpenCV Library.',\n",
       " 'Dr. Dobb’s Journal of Software Tools , 2000.',\n",
       " 'Charlotte Caucheteux, Alexandre Gramfort, and Jean-R ´emi King.',\n",
       " 'Evidence of a predictive coding\\nhierarchy in the human brain listening to speech.',\n",
       " 'Nature human behaviour , 7(3):430–441, 2023.',\n",
       " 'Radoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, and Aude Oliva.',\n",
       " 'Dynamics of scene\\nrepresentations in the human brain revealed by magnetoencephalography and deep neural net-\\nworks.',\n",
       " 'NeuroImage , 153:346–358, 2017.',\n",
       " 'Alexandre D ´efossez, Charlotte Caucheteux, J ´er´emy Rapin, Ori Kabeli, and Jean-R ´emi King.',\n",
       " 'De-\\ncoding speech from non-invasive brain recordings.',\n",
       " 'arXiv preprint arXiv:2208.12266 , 2022.',\n",
       " 'Matteo Ferrante, Tommaso Boccato, and Nicola Toschi.',\n",
       " 'Semantic brain decoding: from fMRI to\\nconceptually similar image reconstruction of visual stimuli.',\n",
       " 'arXiv preprint arXiv:2212.06726 ,\\n2022.',\n",
       " 'Alessandro T Gifford, Kshitij Dwivedi, Gemma Roig, and Radoslaw M Cichy.',\n",
       " 'A large and rich\\nEEG dataset for modeling human visual object recognition.',\n",
       " 'NeuroImage , 264:119754, 2022.',\n",
       " 'Tijl Grootswagers, Amanda K Robinson, and Thomas A Carlson.',\n",
       " 'The representational dynamics of\\nvisual objects in rapid serial visual processing streams.',\n",
       " 'NeuroImage , 188:668–679, 2019.',\n",
       " 'S´ebastien B Hausmann, Alessandro Marin Vargas, Alexander Mathis, and Mackenzie W Mathis.',\n",
       " 'Measuring and modeling the motor system with machine learning.',\n",
       " 'Current opinion in neurobiol-\\nogy, 70:11–23, 2021.',\n",
       " 'Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wick-\\nlin, and Chris I Baker.',\n",
       " 'THINGS: A database of 1,854 object concepts and more than 26,000\\nnaturalistic object images.',\n",
       " 'PloS one , 14(10):e0223792, 2019.',\n",
       " 'Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis\\nKidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker.',\n",
       " 'THINGS-data, a multi-\\nmodal collection of large-scale datasets for investigating object representations in human brain\\nand behavior.',\n",
       " 'eLife , 12:e82580, feb 2023.',\n",
       " 'ISSN 2050-084X.',\n",
       " 'doi: 10.7554/eLife.82580.',\n",
       " 'URL\\nhttps://doi.org/10.7554/eLife.82580 .',\n",
       " '10 Tomoyasu Horikawa and Yukiyasu Kamitani.',\n",
       " 'Generic decoding of seen and imagined objects using\\nhierarchical visual features.',\n",
       " 'Nature communications , 8(1):15037, 2017.',\n",
       " 'David H Hubel and Torsten N Wiesel.',\n",
       " 'Receptive fields, binocular interaction and functional archi-\\ntecture in the cat’s visual cortex.',\n",
       " 'The Journal of physiology , 160(1):106, 1962.',\n",
       " 'Vinay Jayaram and Alexandre Barachant.',\n",
       " 'MOABB: trustworthy algorithm benchmarking for bcis.',\n",
       " 'Journal of neural engineering , 15(6):066011, 2018.',\n",
       " 'Zhicheng Jiao, Xinbo Gao, Ying Wang, Jie Li, and Haojun Xu.',\n",
       " 'Deep convolutional neural networks\\nfor mental load classification based on EEG data.',\n",
       " 'Pattern Recognition , 76:582–595, 2018.',\n",
       " 'Yukiyasu Kamitani and Frank Tong.',\n",
       " 'Decoding the visual and subjective contents of the human\\nbrain.',\n",
       " 'Nature neuroscience , 8(5):679–685, 2005.',\n",
       " 'Nancy Kanwisher, Josh McDermott, and Marvin M Chun.',\n",
       " 'The fusiform face area: a module in\\nhuman extrastriate cortex specialized for face perception.',\n",
       " 'Journal of neuroscience , 17(11):4302–\\n4311, 1997.',\n",
       " 'Jean-R ´emi King and Valentin Wyart.',\n",
       " 'The human brain encodes a chronicle of visual events at each\\ninstant of time through the multiplexing of traveling waves.',\n",
       " 'Journal of Neuroscience , 41(34):\\n7224–7233, 2021.',\n",
       " 'Diederik P Kingma and Jimmy Ba.',\n",
       " 'Adam: A method for stochastic optimization.',\n",
       " 'arXiv preprint\\narXiv:1412.6980 , 2014.',\n",
       " 'Ren Li, Jared S Johansen, Hamad Ahmed, Thomas V Ilyevsky, Ronnie B Wilbur, Hari M Bharad-\\nwaj, and Jeffrey Mark Siskind.',\n",
       " 'The perils and pitfalls of block design for EEG classification\\nexperiments.',\n",
       " 'IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(1):316–333,\\n2020.',\n",
       " 'Yan Liu, Zehao Zhao, Minpeng Xu, Haiqing Yu, Yanming Zhu, Jie Zhang, Linghao Bu, Xiaoluo\\nZhang, Junfeng Lu, Yuanning Li, et al.',\n",
       " 'Decoding and synthesizing tonal language speech from\\nbrain activity.',\n",
       " 'Science Advances , 9(23):eadh0478, 2023.',\n",
       " 'Weijian Mai and Zhijun Zhang.',\n",
       " 'Unibrain: Unify image reconstruction and captioning all in one\\ndiffusion model from human brain activity.',\n",
       " 'arXiv preprint arXiv:2308.07428 , 2023.',\n",
       " 'Ben McCartney, Barry Devereux, and Jesus Martinez-del Rincon.',\n",
       " 'A zero-shot deep metric learn-\\ning approach to brain–computer interfaces for image retrieval.',\n",
       " 'Knowledge-Based Systems , 246:\\n108556, 2022.',\n",
       " 'Johannes Mehrer, Courtney J Spoerer, Emer C Jones, Nikolaus Kriegeskorte, and Tim C Kietzmann.',\n",
       " 'An ecologically motivated image dataset for deep learning yields better models of human vision.',\n",
       " 'Proceedings of the National Academy of Sciences , 118(8):e2011417118, 2021.',\n",
       " 'Sean L Metzger, Kaylo T Littlejohn, Alexander B Silva, David A Moses, Margaret P Seaton, Ran\\nWang, Maximilian E Dougherty, Jessie R Liu, Peter Wu, Michael A Berger, et al.',\n",
       " 'A high-\\nperformance neuroprosthesis for speech decoding and avatar control.',\n",
       " 'Nature , pp.',\n",
       " '1–10, 2023.',\n",
       " 'David A Moses, Sean L Metzger, Jessie R Liu, Gopala K Anumanchipalli, Joseph G Makin,\\nPengfei F Sun, Josh Chartier, Maximilian E Dougherty, Patricia M Liu, Gary M Abrams, et al.',\n",
       " 'Neuroprosthesis for decoding speech in a paralyzed person with anarthria.',\n",
       " 'New England Journal\\nof Medicine , 385(3):217–227, 2021.',\n",
       " 'Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant.',\n",
       " 'Re-\\nconstructing visual experiences from brain activity evoked by natural movies.',\n",
       " 'Current biology ,\\n21(19):1641–1646, 2011.',\n",
       " 'John O’Keefe and Lynn Nadel.',\n",
       " 'The hippocampus as a cognitive map.',\n",
       " 'Behavioral and Brain Sci-\\nences , 2(4):487–494, 1979.',\n",
       " 'Furkan Ozcelik and Rufin VanRullen.',\n",
       " 'Brain-diffuser: Natural scene reconstruction from fMRI\\nsignals using generative latent diffusion.',\n",
       " 'arXiv preprint arXiv:2303.05334 , 2023.',\n",
       " '11 Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and\\nMubarak Shah.',\n",
       " 'Decoding brain representations by multimodal learning of neural activity and\\nvisual features.',\n",
       " 'IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(11):3833–\\n3849, 2020.',\n",
       " 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever.',\n",
       " 'Learning transferable visual models from natural language supervision, 2021.',\n",
       " 'Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Joce-\\nlyn Faubert.',\n",
       " 'Deep learning-based electroencephalography analysis: a systematic review.',\n",
       " 'Journal\\nof neural engineering , 16(5):051001, 2019.',\n",
       " 'Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\\nJoshua Tenenbaum, and Evelina Fedorenko.',\n",
       " 'Artificial neural networks accurately predict lan-\\nguage processing in the brain.',\n",
       " 'BioRxiv , pp.',\n",
       " '2020–06, 2020.',\n",
       " 'Paul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen,\\nAidan J Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, et al.',\n",
       " 'Reconstructing\\nthe mind’s eye: fMRI-to-image with contrastive learning and diffusion priors.',\n",
       " 'arXiv preprint\\narXiv:2305.18274 , 2023.',\n",
       " 'Katja Seeliger, Umut G ¨uc ¸l¨u, Luca Ambrogioni, Yagmur G ¨uc ¸l¨ut¨urk, and Marcel AJ van Gerven.',\n",
       " 'Generative adversarial networks for reconstructing natural images from brain activity.',\n",
       " 'NeuroIm-\\nage, 181:775–785, 2018.',\n",
       " 'Yu Takagi and Shinji Nishimoto.',\n",
       " 'High-resolution image reconstruction with latent diffusion models\\nfrom human brain activity.',\n",
       " 'bioRxiv , 2023. doi: 10.1101/2022.11.18.517004.',\n",
       " 'URL https:\\n//www.biorxiv.org/content/early/2023/03/11/2022.11.18.517004 .',\n",
       " 'Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth.',\n",
       " 'Semantic reconstruction of con-\\ntinuous language from non-invasive brain recordings.',\n",
       " 'Nature Neuroscience , pp.',\n",
       " '1–9, 2023.',\n",
       " 'Armin Thomas, Christopher R ´e, and Russell Poldrack.',\n",
       " 'Self-supervised learning of brain dynamics\\nfrom broad neuroimaging data.',\n",
       " 'Advances in Neural Information Processing Systems , 35:21255–\\n21269, 2022.',\n",
       " 'Stefan Van der Walt, Johannes L Sch ¨onberger, Juan Nunez-Iglesias, Franc ¸ois Boulogne, Joshua D\\nWarner, Neil Yager, Emmanuelle Gouillart, and Tony Yu.',\n",
       " 'scikit-image: image processing in\\npython.',\n",
       " 'PeerJ , 2:e453, 2014.',\n",
       " 'Rufin VanRullen and Leila Reddy.',\n",
       " 'Reconstructing faces from fMRI patterns using deep generative\\nneural networks.',\n",
       " 'Communications biology , 2(1):193, 2019.',\n",
       " 'Francis R Willett, Erin M Kunz, Chaofei Fan, Donald T Avansino, Guy H Wilson, Eun Young\\nChoi, Foram Kamdar, Matthew F Glasser, Leigh R Hochberg, Shaul Druckmann, et al.',\n",
       " 'A high-\\nperformance speech neuroprosthesis.',\n",
       " 'Nature , pp.',\n",
       " '1–6, 2023.',\n",
       " 'Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J\\nDiCarlo.',\n",
       " 'Performance-optimized hierarchical models predict neural responses in higher visual\\ncortex.',\n",
       " 'Proceedings of the national academy of sciences , 111(23):8619–8624, 2014.',\n",
       " 'Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang\\nLiu, and Baochang Zhang.',\n",
       " 'Controllable mind visual diffusion model.',\n",
       " 'arXiv preprint\\narXiv:2305.10135 , 2023.',\n",
       " '12 A A PPENDIX\\nA.1 L INEAR RIDGE REGRESSION SCORES ON PRETRAINED IMAGE REPRESENTATIONS\\nWe provide a (5-fold cross-validated) Ridge regression baseline (Table S1) for comparison with our\\nbrain module results of Section 3, showing considerable improvements for the latter.',\n",
       " 'Table S1: Image retrieval performance of a linear Ridge regression baseline on pretrained image\\nrepresentations\\nTop-5 acc (%) ↑ Median relative rank ↓\\nLatent kind Latent name Small set Large set Small set Large set\\nText/Image\\nalignmentCLIP-Vision (CLS) 10.5 0.50 0.23 0.34\\nCLIP-Text (mean) 6.0 0.25 0.42 0.43\\nCLIP-Vision (mean) 5.5 0.46 0.32 0.37\\nFeature\\nengineeringColor histogram 7.0 0.33 0.31 0.40\\nLocal binary patterns (LBP) 3.5 0.37 0.34 0.44\\nFFT 2D (as real) 4.5 0.46 0.40 0.45\\nHOG 3.0 0.42 0.45 0.46\\nFFT 2D (log-PSD and angle) 2.0 0.37 0.47 0.46\\nVariational\\nautoencoderAutoKL 7.5 0.54 0.24 0.38\\nVDV AE 8.0 0.50 0.33 0.43\\nSelf-supervised\\nlearning DINOv2 (CLS) 7.5 0.46 0.25 0.35\\nSupervisedVGG-19 12.5 1.04 0.18 0.33\\nResNet-101 4.0 0.37 0.36 0.42\\nDenseNet-201 5.0 0.29 0.39 0.45\\nWide ResNet-101-2 3.5 0.42 0.40 0.46\\nMobileNet v3 3.5 0.42 0.40 0.42\\nA.2 H YPERPARAMETER SEARCH\\nWe run a hyperparameter search to find an appropriate configuration (MEG preprocessing, opti-\\nmizer, brain module architecture and loss definition) for the MEG-to-image retrieval task ( λ= 0).',\n",
       " 'We randomly split the 79,392 (MEG, image) pairs of the adapted training set (Section 2.8) into\\n60%-20%-20% train, valid and test splits such that all presentations of a given image are contained\\nin the same split.',\n",
       " 'We use the validation split to perform early stopping and the test split to evaluate\\nthe performance of a configuration.',\n",
       " 'For the purpose of this search we pick CLIP-Vision (CLS) latent as a representative latent, since\\nit achieved good retrieval performance in preliminary experiments.',\n",
       " 'We run the search six times\\nusing two different random seed initializations for the brain module and three different random\\ntrain/valid/test splits.',\n",
       " 'Fig.',\n",
       " 'S1 summarizes the results of this hyperparameter search.',\n",
       " 'Based on this search, we use the following configuration: MEG window (tmin, tmax)of\\n[−0.5,1.0]s, learning rate of 3×10−4, batch size of 128, brain module with two convolutional\\nblocks and both the spatial attention and subject layers of D ´efossez et al.',\n",
       " '(2022), affine projection\\ntemporal aggregation layer with a single block in the CLIP projection head, and full CLIP loss (in-\\ncluding learned temperature parameter, normalization along both axes and symmetric terms).',\n",
       " 'The\\nfinal architecture configuration is presented in Table S2.',\n",
       " 'A.3 F ULL-WINDOW MEG- BASED IMAGE RETRIEVALS\\nFig.',\n",
       " 'S2 shows examples of retrieved images based on the best performing latents identified in Sec-\\ntion 3.',\n",
       " '13 Figure S1: Hyperparameter search results for the MEG-to-image retrieval task, presenting the impact\\nof (A) optimizer learning rate and batch size, ( B) number of convolutional blocks and use of spatial\\nattention and/or subject-specific layers in the brain module, ( C) MEG window parameters, ( D) type\\nof temporal aggregation layer and number of blocks in the CLIP projection head of the brain module,\\nand ( E) CLIP loss configuration (normalization axes, use of learned temperature parameter and use\\nof symmetric terms).',\n",
       " 'Chance-level performance top-5 accuracy is 0.05%.',\n",
       " 'Table S2: Brain module configuration adapted from D ´efossez et al.',\n",
       " '(2022) for use with a target latent\\nof size 768 ( e.g.',\n",
       " 'CLIP-Vision (CLS), see Section 2.4) in retrieval settings.',\n",
       " 'Layer Input shape Output shape # parameters\\nSpatial attention block (272, 181) (270, 181) 552,960\\nLinear projection (270, 181) (270, 181) 73,170\\nSubject-specific linear layer (270, 181) (270, 181) 291,600\\nResidual dilated conv block 1 (270, 181) (320, 181) 1,183,360\\nResidual dilated conv block 2 (320, 181) (320, 181) 1,231,360\\nLinear projection (320, 181) (2048, 181) 1,518,208\\nTemporal aggregation (2048, 181) (2048, 1) 182\\nMLP projector (2048, 1) (768, 1) 1,573,632\\nTotal 6,424,472\\n14 Figure S2: Representative examples of retrievals (top-4) using models trained on full windows (from\\n-0.5 s to 1 s after image onset).',\n",
       " 'Retrieval set: N= 6,059images from 1,196categories.',\n",
       " 'A.4 I MAGE EMBEDDINGS\\nWe evaluate the performance of linear baselines and of a deep convolutional neural network on the\\nMEG-to-image retrieval task using a set of classic visual embeddings.',\n",
       " 'We grouped these embeddings\\nby their corresponding paradigm:\\nSupervised learning.',\n",
       " 'DenseNet-121,DenseNet-169,DenseNet-201, MobileNet v2, MobileNet\\nv3, ResNet-101, ResNet-18, ResNet-50, ResNext101-32-8d, ResNext50-32-4d, VGG-16,VGG-19,\\nWide ResNet-101-2, Wide ResNet-50-2.',\n",
       " 'Text/Image alignment.',\n",
       " 'CLIP-Vision, CLIP-Text, and their CLS and MEAN pooling.',\n",
       " 'Self-supervised learning.',\n",
       " 'DINOv1, DINOv2 and their CLS and MEAN pooling.',\n",
       " '15 Figure S3: Examples of generated images conditioned on fMRI-based latent predictions.',\n",
       " 'The groups\\nof three stacked rows represent best, average and worst retrievals, as evaluated by the sum of (minus)\\nSwA V and SSIM.',\n",
       " 'Variational autoencoders.',\n",
       " 'The activations of the 31 first layers of the very deep variational-\\nautoencoder (VDV AE), and the Kullback-Leibler variational-autoencoder (AutoKL) used in the gen-\\nerative module (Section 2.5).',\n",
       " 'Engineered features.',\n",
       " 'The color histogram of the seen image (8 bins per channels); the local binary\\npatterns (LBP) using the implementation in OpenCV 2 (Bradski, 2000) with ’uniform’ method,\\nP= 8 andR= 1; the Histogram of Oriented Gradients (HOG) using the implementation of sk-\\nimage (Van der Walt et al., 2014) with 8 orientations, 8 pixels-per-cell and 2 cells-per-block.',\n",
       " 'A.5 7T FMRI DATASET\\nThe Natural Scenes Dataset (NSD) (Allen et al., 2022) contains fMRI data from 8 participants\\nviewing a total of 73,000 RGB images.',\n",
       " 'It has been successfully used for reconstructing seen images\\nfrom fMRI in several studies (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023; Scotti et al.,\\n2023).',\n",
       " 'In particular, these studies use a highly preprocessed, compact version of fMRI data (“betas”)\\nobtained through generalized linear models fitted across multiple repetitions of the same image.',\n",
       " 'Each participant saw a total of 10,000 unique images (repeated 3 times each) across 37 sessions.',\n",
       " 'Each session consisted in 12 runs of 5 minutes each, where each image was seen during 3 s, with\\na 1-s blank interval between two successive image presentations.',\n",
       " 'Among the 8 participants, only 4\\n(namely 1, 2, 5 and 7) completed all sessions.',\n",
       " 'To compute the three latents used to reconstruct the seen images from fMRI data (as described in\\nSection 2.5) we follow Ozcelik & VanRullen (2023) and train and evaluate three distinct Ridge\\nregression models using the exact same split.',\n",
       " 'That is, for each of the four remaining participants,\\nthe 9,000 uniquely-seen-per-participant images (and their three repetitions) are used for training,\\nand a common set of 1000 images seen by all participant is kept for evaluation (also with their three\\nrepetitions).',\n",
       " 'We report reconstructions and metrics for participant 1.',\n",
       " 'Theαcoefficient for the L2-regularization of the regressions are cross-validated with a 5-fold\\nscheme on the training set of each subject.',\n",
       " 'We follow the same standardization scheme for inputs\\nand predictions as in (Ozcelik & VanRullen, 2023).',\n",
       " 'Fig.',\n",
       " 'S3 presents generated images obtained using the NSD dataset (Allen et al., 2022).',\n",
       " '16']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "sentences = split_text_into_sentences(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Sentence Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BRAIN DECODING :TOWARD REAL -TIME\n",
       " RECONSTRUCTION OF VISUAL PERCEPTION\n",
       " Yohann Benchetrit1,∗, Hubert Banville1,∗, Jean-R ´emi King1,2\n",
       " 1FAIR, Meta,2Laboratoire des Syst `emes Perceptifs, ´Ecole Normale Sup ´erieure, PSL University\n",
       " {ybenchetrit,hubertjb,jeanremi }@meta.com\n",
       " ABSTRACT\n",
       " In the past five years, the use of generative and foundational AI systems has\n",
       " greatly improved the decoding of brain activity.,\n",
       " Visual perception, in particular,\n",
       " can now be decoded from functional Magnetic Resonance Imaging (fMRI) with\n",
       " remarkable fidelity.,\n",
       " This neuroimaging technique, however, suffers from a lim-\n",
       " ited temporal resolution ( ≈0.5 Hz) and thus fundamentally constrains its real-time\n",
       " usage.,\n",
       " Here, we propose an alternative approach based on magnetoencephalog-\n",
       " raphy (MEG), a neuroimaging device capable of measuring brain activity with\n",
       " high temporal resolution ( ≈5,000 Hz).,\n",
       " For this, we develop an MEG decoding\n",
       " model trained with both contrastive and regression objectives and consisting of\n",
       " three modules: i) pretrained embeddings obtained from the image, ii) an MEG\n",
       " module trained end-to-end and iii) a pretrained image generator.,\n",
       " Our results are\n",
       " threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval\n",
       " over classic linear decoders.,\n",
       " Second, late brain responses to images are best de-\n",
       " coded with DINOv2, a recent foundational image model.,\n",
       " Third, image retrievals\n",
       " and generations both suggest that MEG signals primarily contain high-level visual\n",
       " features, whereas the same approach applied to 7T fMRI also recovers low-level\n",
       " features.,\n",
       " Overall, these results provide an important step towards the decoding\n",
       " – in real time – of the visual processes continuously unfolding within the human\n",
       " brain.,\n",
       " 1 I NTRODUCTION\n",
       " Automating the discovery of brain representations.,\n",
       " Understanding how the human brain rep-\n",
       " resents the world is arguably one of the most profound scientific challenges.,\n",
       " This quest, which\n",
       " originally consisted of searching, one by one, for the specific features that trigger each neuron, ( e.g.\n",
       " Hubel & Wiesel (1962); O’Keefe & Nadel (1979); Kanwisher et al. (1997)), is now being automated\n",
       " by Machine Learning (ML) in two mains ways.,\n",
       " First, as a signal processing tool, ML algorithms are\n",
       " trained to extract informative patterns of brain activity in a data-driven manner.,\n",
       " For example, Kami-\n",
       " tani & Tong (2005) trained a support vector machine to classify the orientations of visual gratings\n",
       " from functional Magnetic Resonance Imaging (fMRI).,\n",
       " Since then, deep learning has been increas-\n",
       " ingly used to discover such brain activity patterns (Roy et al., 2019; Thomas et al., 2022; Jayaram\n",
       " & Barachant, 2018; D ´efossez et al., 2022; Scotti et al., 2023).,\n",
       " Second, ML algorithms are used as\n",
       " functional models of the brain.,\n",
       " For example, Yamins et al. (2014) have shown that the embedding\n",
       " of natural images in pretrained deep nets linearly account for the neuronal responses to these images\n",
       " in the cortex.,\n",
       " Since, pretrained deep learning models have been shown to account for a wide variety\n",
       " of stimuli including text, speech, navigation, and motor movement (Banino et al., 2018; Schrimpf\n",
       " et al., 2020; Hausmann et al., 2021; Mehrer et al., 2021; Caucheteux et al., 2023).,\n",
       " Generating images from brain activity.,\n",
       " This observed representational alignment between brain\n",
       " activity and deep learning models creates a new opportunity: Decoding of visual stimuli need not\n",
       " be restricted to a limited set of classes, but can now leverage pretrained representations to condi-\n",
       " tion subsequent generative AI models.,\n",
       " While the resulting image may be partly “hallucinated”,\n",
       " interpreting images can be much simpler than interpreting latent features.,\n",
       " Following a long series\n",
       " ∗Equal contribution.\n",
       " 1 of generative approaches (Nishimoto et al., 2011; Kamitani & Tong, 2005; VanRullen & Reddy,\n",
       " 2019; Seeliger et al., 2018), diffusion techniques have, in this regard, significantly improved the\n",
       " generation of images from functional Magnetic Resonance Imaging (fMRI).,\n",
       " The resulting pipeline\n",
       " typically consists of three main modules: (1) a set of pretrained embeddings obtained from the im-\n",
       " age onto which (2) fMRI activity can be linearly mapped and (3) ultimately used to condition a\n",
       " pretrained image-generation model (Ozcelik & VanRullen, 2023; Mai & Zhang, 2023; Zeng et al.,\n",
       " 2023; Ferrante et al., 2022).,\n",
       " These recent fMRI studies primarily differ in the type of pretrained\n",
       " image-generation model that they use.,\n",
       " The challenge of real-time decoding.,\n",
       " This generative decoding approach has been mainly applied\n",
       " to fMRI.,\n",
       " However, the temporal resolution of fMRI is limited by the time scale of blood flow and\n",
       " typically leads to one snapshot of brain activity every two seconds – a time scale that challenges its\n",
       " clinical usage, e.g. for patients who require a brain-computer-interface (Willett et al., 2023; Moses\n",
       " et al., 2021; Metzger et al., 2023; D ´efossez et al., 2022).,\n",
       " On the contrary, magnetoencephalography\n",
       " (MEG) can measure brain activity at a much higher temporal resolution ( ≈5,000 Hz) by recording\n",
       " the fluctuation of magnetic fields elicited by the post-synaptic potentials of pyramidal neurons.,\n",
       " This\n",
       " higher temporal resolution comes at cost, however: the spatial resolution of MEG is limited to ≈300\n",
       " sensors, whereas fMRI measures ≈100,000 voxels.,\n",
       " In sum, fMRI intrinsically limits our ability to\n",
       " (1) track the dynamics of neuronal activity, (2) decode dynamic stimuli (speech, videos etc) and\n",
       " (3) apply these tools to real-time use cases.,\n",
       " Conversely, it is unknown whether temporally-resolved\n",
       " neuroimaging systems like MEG are sufficiently precise to generate natural images in real-time.,\n",
       " Our approach.,\n",
       " Combining previous work on speech retrieval from MEG (D ´efossez et al., 2022)\n",
       " and on image generation from fMRI (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023),\n",
       " we here develop a three-module pipeline trained to (1) align MEG activity onto pretrained visual\n",
       " embeddings and (2) generate images from a stream of MEG signals (Fig. 1).,\n",
       " Figure 1: ( A) Approach.,\n",
       " Locks indicate pretrained models.,\n",
       " ( B) Processing schemes.,\n",
       " Unlike image\n",
       " generation, image retrieval can be done in the aligned latent space, but requires the true image in the\n",
       " retrieval set.,\n",
       " Our systematic benchmark provides two main contributions: our MEG decoder leads to (1) high-\n",
       " performing image retrieval and image generation, (2) new means to interpret the unfolding of visual\n",
       " processing in the brain.,\n",
       " This demonstrates the capacity of our approach to truly generalize to new\n",
       " visual concepts, paving the way to “free-form” visual decoding.,\n",
       " Overall, our findings outline a\n",
       " promising avenue for real-time decoding of visual representations in the lab and in the clinic.,\n",
       " 2 2 M ETHODS\n",
       " 2.1 P ROBLEM STATEMENT,\n",
       " We aim to decode images from multivariate time series of brain activity recorded with MEG as\n",
       " healthy participants watched a sequence of natural images.,\n",
       " Let Xi∈RC×Tbe the MEG time\n",
       " window collected as an image Iiwas presented to the participant, where Cis the number of MEG\n",
       " channels, Tis the number of time points in the MEG window and i∈[ [1, N] ].,\n",
       " Letzi∈RFbe the\n",
       " latent representation of Ii, with Fthe number of features, obtained by embedding the image using\n",
       " a pretrained image model (Section 2.4).,\n",
       " As described in more detail below, our decoding approach\n",
       " relies on training a brain module fθ:RC×T→RFto maximally retrieve or predict Iithrough zi,\n",
       " given Xi.\n",
       " 2.2 T RAINING OBJECTIVES,\n",
       " We use different training objectives for the different parts of our proposed pipeline.,\n",
       " First, in the case\n",
       " of retrieval, we aim to pick the right image Ii(i.e., the one corresponding to Xi) out of a bank of\n",
       " candidate images.,\n",
       " To do so, we train fθusing the CLIP loss (Radford et al., 2021) on batches of size\n",
       " B with exactly one positive example:\n",
       " LCLIP (θ) =−1\n",
       " BBX\n",
       " i=1 \n",
       " logexp(s(ˆzi,zi)/τ)PB\n",
       " j=1exp(s(ˆzi,zj)/τ)+ logexp(s(ˆzi,zi)/τ)PB\n",
       " k=1exp(s(ˆzk,zi)/τ)!,\n",
       " (1)\n",
       " where sis the cosine similarity, ziandˆzi=fθ(Xi)are the latent representation and the correspond-\n",
       " ing MEG-based prediction, respectively, and τis a learned temperature parameter.,\n",
       " Next, to go beyond retrieval and instead generate images, we train fθto directly predict the latent\n",
       " representations zsuch that we can use them to condition generative image models.,\n",
       " This is done\n",
       " using a standard mean squared error (MSE) loss:\n",
       " LMSE(θ),\n",
       " =1\n",
       " NFNX\n",
       " i=1∥zi−ˆzi∥2\n",
       " 2 (2),\n",
       " Finally, we combine the CLIP and MSE losses using a convex combination with tuned weight to\n",
       " train models that benefit from both training objectives:\n",
       " LCombined =λLCLIP + (1−λ)LMSE (3)\n",
       " 2.3 B RAIN MODULE,\n",
       " We adapt the dilated residual ConvNet architecture of D ´efossez et al. (2022), denoted as fθ, to\n",
       " learn the projection from an MEG window Xi∈RC×Tto a latent image representation zi∈RF.,\n",
       " The original model’s output ˆYbackbone ∈RF′×Tmaintains the temporal dimension of the network\n",
       " through its residual blocks.,\n",
       " However, here we regress a single latent per input instead of a sequence\n",
       " ofTlatents like in D ´efossez et al. (2022).,\n",
       " Consequently, we add a temporal aggregation layer\n",
       " to reduce the temporal dimension of ˆYbackbone to obtain ˆyagg∈RF′.,\n",
       " We experiment with three\n",
       " types of aggregations: global average pooling, a learned affine projection, and an attention layer.,\n",
       " Finally, we add two MLP heads1,i.e., one for each term in LCombined , to project from F′to the F\n",
       " dimensions of the target latent.,\n",
       " We run a hyperparameter search to identify an appropriate configuration of preprocessing, brain\n",
       " module architecture, optimizer and loss hyperparameters for the retrieval task (see Appendix A.2).,\n",
       " The final architecture configuration for retrieval is described in Table S2 and contains e.g. 6.4M\n",
       " trainable parameters for F= 768 .,\n",
       " 1A head consists of repeated LayerNorm-GELU-Linear blocks.,\n",
       " 3,\n",
       " For image generation experiments, the output of the MSE head is further postprocessed as in Ozcelik\n",
       " & VanRullen (2023), i.e., we z-score normalize each feature across predictions, and then apply the\n",
       " inverse z-score transform fitted on the training set (defined by the mean and standard deviation of\n",
       " each feature dimension on the target embeddings).,\n",
       " We select λinLCombined by sweeping over\n",
       " {0.0,0.25,0.5,0.75,1.0}and pick the model whose top-5 accuracy is the highest on the large test\n",
       " set.,\n",
       " Of note, when training models to generate CLIP and AutoKL latents, we simplify the task of\n",
       " the CLIP head by reducing the dimensionality of its target: we use the CLS token for CLIP-Vision\n",
       " (FMSE = 768 ), the ”mean” token for CLIP-Text ( FMSE = 768 ), and the channel-average for\n",
       " AutoKL latents ( FMSE = 4096 ), respectively.,\n",
       " 2.4 I MAGE MODULES,\n",
       " We study the functional alignment between brain activity and a variety of (output) embeddings\n",
       " obtained from deep neural networks trained in three different representation learning paradigms,\n",
       " spanning a wide range of dimensionalities: supervised learning ( e.g. VGG-19), image-text align-\n",
       " ment (CLIP), and variational autoencoders.,\n",
       " When using vision transformers, we further include two\n",
       " additional embeddings of smaller dimensionality: the average of all output embeddings across to-\n",
       " kens (mean), and the output embedding of the class-token (CLS).,\n",
       " For comparison, we also evaluate\n",
       " our approach on human-engineered features obtained without deep learning.,\n",
       " The list of embeddings\n",
       " is provided in Appendix A.4.,\n",
       " For clarity, we focus our experiments on a representative subset.,\n",
       " 2.5 G ENERATION MODULE\n",
       " To fairly compare our work to the results obtained with fMRI results, we follow the approach of\n",
       " Ozcelik & VanRullen (2023) and use a model trained to generate images from pretrained embed-\n",
       " dings.,\n",
       " Specifically, we use a latent diffusion model conditioned on three embeddings: CLIP-Vision\n",
       " (257tokens ×768), CLIP-Text ( 77tokens ×768), and a variational autoencoder latent (AutoKL;\n",
       " (4×64×64).,\n",
       " Following Ozcelik & VanRullen (2023), we apply diffusion with 50 DDIM steps, a\n",
       " guidance of 7.5, a strength of 0.75 with respect to the image-to-image pipeline, and a mixing of 0.4.\n",
       " 2.6 T RAINING AND COMPUTATIONAL CONSIDERATIONS\n",
       " Cross-participant models are trained on a set of ≈63,000 examples using the Adam optimizer\n",
       " (Kingma & Ba, 2014) with learning rate of 3×10−4and a batch size of 128.,\n",
       " We use early stopping\n",
       " on a validation set of ≈15,800examples randomly sampled from the original training set, with a\n",
       " patience of 10, and evaluate the performance of the model on a held-out test set (see below).,\n",
       " Models\n",
       " are trained on a single V olta GPU with 32 GB of memory.,\n",
       " We train each model three times using\n",
       " three different random seeds for the weight initialization of the brain module.,\n",
       " 2.7 E VALUATION\n",
       " Retrieval metrics.,\n",
       " We first evaluate decoding performance using retrieval metrics.,\n",
       " For a known\n",
       " test set, we are interested in the probability of identifying the correct image given the model predic-\n",
       " tions.,\n",
       " Retrieval metrics have the advantage of sharing the same scale regardless of the dimensional-\n",
       " ity of the MEG (like encoding metrics), the dimensionality of the image embedding (like regression\n",
       " metrics).,\n",
       " We evaluate retrieval using either the relative median rank (which does not depend on the\n",
       " size of the retrieval set), defined as the rank of a prediction divided by the size of the retrieval set, or\n",
       " thetop-5 accuracy (which is more common in the literature).,\n",
       " Generation metrics.,\n",
       " Decoding performance is often measured qualitatively as well as quantita-\n",
       " tively using a variety of metrics reflecting the reconstruction fidelity both in terms of perception and\n",
       " semantics.,\n",
       " For fair comparison with fMRI generations, we provide the same metrics as Ozcelik &\n",
       " VanRullen (2023), computed between seen and generated images: PixCorr (the pixel-wise correla-\n",
       " tion between the true and generated images), SSIM (Structural Similarity Index Metric), and SwA V\n",
       " (the correlation with respect to SwA V-ResNet50 output).,\n",
       " On the other hand, AlexNet(2/5), Incep-\n",
       " tion, and CLIP are the respective 2-way comparison scores of layers 2/5 of AlexNet, the pooled last\n",
       " layer of Inception and the output layer of CLIP.,\n",
       " For the NSD dataset, these metrics are reported for\n",
       " participant 1 only (see Appendix A.5).,\n",
       " 4 To avoid non-representative cherry-picking, we sort all generations on the test set according to the\n",
       " sum of (minus) SwA V and SSIM.,\n",
       " We then split the data into 15 blocks and pick 4 images from the\n",
       " best, middle and worst blocks with respect to the summed metric.,\n",
       " Real-time and average metrics.,\n",
       " It is common in fMRI to decode brain activity from preprocessed\n",
       " values estimated with a General Linear Model.,\n",
       " These “beta values” are estimates of brain responses\n",
       " to individual images, computed across multiple repetitions of such images.,\n",
       " To provide a fair as-\n",
       " sessment of possible MEG decoding performance, we thus leverage repeated image presentations\n",
       " available in the datasets (see below) by averaging predictions before evaluating metrics.,\n",
       " 2.8 D ATASET\n",
       " We test our approach on the “THINGS-MEG” dataset (Hebart et al., 2023).,\n",
       " Four participants (2\n",
       " females, 2 males; mean age of 23.25 years), underwent 12 MEG sessions during which they were\n",
       " presented with a set of 22,448 unique images selected from the THINGS database (Hebart et al.,\n",
       " 2019), covering 1,854 categories.,\n",
       " Of those, only a subset of 200 images (each one of a different cat-\n",
       " egory) was shown multiple times to the participants.,\n",
       " The images were displayed for 500 ms each,\n",
       " with a variable fixation period of 1000±200ms between presentations.,\n",
       " The THINGS dataset ad-\n",
       " ditionally contains 3,659 images that were not shown to the participants and that we use to augment\n",
       " the size of our retrieval set and emphasize the robustness of our method.,\n",
       " MEG Preprocessing.,\n",
       " We use a minimal MEG data-preprocessing pipeline as in D ´efossez et al.\n",
       " (2022).,\n",
       " Raw data from the 272 MEG radial gradiometer channels is downsampled from 1,200 Hz to\n",
       " 120 Hz before being centered and clipped channel-wise above +/- 5 standard errors.,\n",
       " The continuous\n",
       " MEG data is then epoched from -500 ms to 1,000 ms relative to stimulus onset.,\n",
       " Finally, baseline-\n",
       " correction is performed by subtracting the mean signal value observed between the start of an epoch\n",
       " and the stimulus onset for each channel.,\n",
       " Splits.,\n",
       " The original split of Hebart et al. (2023) consists of 22,248 uniquely presented images,\n",
       " and 200 test images repeated 12 times each for each participant ( i.e., 2,400 trials per participant).,\n",
       " The use of this data split presents a challenge, however, as the test set contains only one image\n",
       " per category, and these categories are also seen in the training set.,\n",
       " This means evaluating retrieval\n",
       " performance on this test set does not measure the capacity of the model to (1) extrapolate to new\n",
       " unseen categories of images and (2) recover a particular image within a set of multiple images of\n",
       " the same category, but rather only to “categorize” it.,\n",
       " Consequently, we propose two modifications\n",
       " of the original split.,\n",
       " First, we remove from the training set any image whose category appears in the\n",
       " original test set.,\n",
       " This “adapted training set” removes any categorical leakage across the train/test\n",
       " split and makes it possible to assess the capacity of the model to decode images of unseen image\n",
       " categories ( i.e., a “zero-shot” setting).,\n",
       " Second, we propose a new “large test set” that is built using\n",
       " the images removed from the training set.,\n",
       " This new test set effectively allows evaluating retrieval\n",
       " performance of images within images of the same category2.,\n",
       " We report results on both the original\n",
       " (“small”) and the “large” test sets to enable comparisons with the original settings of Ozcelik &\n",
       " VanRullen (2023).,\n",
       " Finally, we also compare our results to the performance obtained by a similar\n",
       " pipeline but trained on fMRI data using the NSD dataset (Allen et al., 2022) (see Appendix A.5).,\n",
       " 3 R ESULTS\n",
       " ML as an effective model of the brain.,\n",
       " Which representations of natural images are likely to\n",
       " maximize decoding performance?,\n",
       " To answer this question, we compare the retrieval performance\n",
       " obtained by linear Ridge regression models trained to predict one of 16 different latent visual rep-\n",
       " resentations given the flattened MEG response Xito each image Ii(Table S1).,\n",
       " While all image\n",
       " embeddings lead to above-chance retrieval, supervised and text/image alignment models ( e.g. VGG,\n",
       " CLIP) yield the highest retrieval scores.,\n",
       " 2We leave out images of the original test set from this new large test set, as keeping them would create a\n",
       " discrepancy between the number of MEG repetitions for training images and test images.,\n",
       " 5 ML as an effective toolto learn brain responses.,\n",
       " We then compare these linear baselines to\n",
       " a deep ConvNet architecture (D ´efossez et al., 2022) trained on the same task3,i.e., to retrieve the\n",
       " matching image given an MEG window.,\n",
       " Using a deep model leads to a 7X improvement over the lin-\n",
       " ear baselines (Fig. 2).,\n",
       " Multiple types of image embeddings lead to good retrieval performance, with\n",
       " VGG-19 (supervised learning), CLIP-Vision (text/image alignment) and DINOv2 (self-supervised\n",
       " learning) yielding top-5 accuracies of 70.33±2.80%,68.66±2.84%,68.00±2.86%, respectively\n",
       " (where the standard error of the mean is computed across the averaged image-wise metrics).,\n",
       " Similar\n",
       " conclusions, although with lower performance, can be drawn from our “large” test set setting, where\n",
       " decoding cannot rely solely on the image category but also requires discriminating between multiple\n",
       " images of the same category.,\n",
       " Representative retrieval examples are shown in Appendix A.3.\n",
       " Figure 2: Image retrieval performance obtained from a trained deep ConvNet.,\n",
       " The original “small”\n",
       " test set (Hebart et al., 2023) comprises 200 distinct images, each belonging to a different category.,\n",
       " In contrast, our proposed “large” test set comprises 12 images from each of those 200 categories,\n",
       " yielding a total of 2,400 images.,\n",
       " Chance-level is 2.5% top-5 accuracy for the small test set and\n",
       " 0.21% for the large test set.,\n",
       " The best latent representations yield accuracies around 70% and 13%\n",
       " for the small and large test sets, respectively.,\n",
       " Temporally-resolved image retrieval.,\n",
       " The above results are obtained from the full time window\n",
       " (-500 ms to 1,000 ms relative to stimulus onset).,\n",
       " To further investigate the possibility of decoding\n",
       " visual representations as they unfold in the brain, we repeat this analysis on 250 ms-long sliding win-\n",
       " dows (Fig. 3).,\n",
       " For clarity, we focus on a subset of representative image embeddings.,\n",
       " As expected,\n",
       " all models yield chance-level performance before the image presentation.,\n",
       " For all models, a first\n",
       " clear peak can then be observed on the 0 to 250-ms window, followed by a second peak, after the\n",
       " image offset, which then quickly goes back to chance-level.,\n",
       " Interestingly, the recent self-supervised\n",
       " model DINOv2 yields particularly good retrieval performance after the image offset.,\n",
       " To get a better sense of what the above decoding metrics mean, we present the top-1 retrieved images\n",
       " from an augmented retrieval set built by concatenating the “large” test set with an additional set of\n",
       " 3,659 images that were not seen by the participants (Fig. 4).,\n",
       " Overall, the retrieved images tend to come from the correct category, such as “speaker” or “brocoli”,\n",
       " mostly during the first few sub-windows ( t≤1s).,\n",
       " However, these retrieved images do not appear\n",
       " to share obvious low-level features to the images seen by the participants.,\n",
       " 3We use λ= 1inLCombined as we are solely concerned with the retrieval part of the pipeline here.,\n",
       " 6 Figure 3: Retrieval performance of models trained on 250 ms sliding windows for different image\n",
       " embeddings.,\n",
       " The shaded gray area indicates the 0.5-s interval during which the image was presented\n",
       " to the participants.,\n",
       " Accuracy generally peaked right after the image onset and offset.,\n",
       " Figure 4: Representative examples of dynamic retrievals using CLIP-Vision (CLS) and models\n",
       " trained on 250-ms sliding windows (Image onset: t= 0, retrieval set: N= 6,059from 1,196\n",
       " categories).,\n",
       " The groups of three stacked rows represent best, average and worst retrievals, obtained\n",
       " by sampling examples from the <10%, 45-55% and >90% percentile groups based on top-5 accu-\n",
       " racy.,\n",
       " Overall, and while further analyses of these results remain necessary, it seems that (1) our decoding\n",
       " leverages the brain responses related to both the onset and the offset of the image and (2) category-\n",
       " level information dominates these visual representations as early as 250 ms.\n",
       " 7 Table 1: Quantitative evaluation of reconstruction quality from MEG data on THINGS-MEG (com-\n",
       " pared to fMRI data on NSD (Allen et al., 2022) using a cross-validated Ridge regression).,\n",
       " We re-\n",
       " port PixCorr, SSIM, AlexNet(2), AlexNet(5), Inception, SwA V and CLIP (the side-arrow indicates\n",
       " whether better scores are higher or lower).,\n",
       " In particular, this shows that fMRI betas as provided in\n",
       " NSD are significantly easier to decode than MEG signals from THINGS-MEG.,\n",
       " Low-level High-level\n",
       " Dataset PixCorr ↑SSIM↑AlexNet(2) ↑AlexNet(5) ↑Inception,\n",
       " ↑CLIP↑SwA V ↓\n",
       " NSD (fMRI) 0.305,\n",
       " 0.366 0.962 0.977 0.910 0.917 0.410\n",
       " THINGS-MEG\n",
       " (per-trial average)0.079 0.329 0.718 0.823 0.674 0.765 0.595\n",
       " THINGS-MEG\n",
       " (per-subject average)0.088 0.333 0.747 0.855 0.712 0.804 0.576\n",
       " THINGS-MEG\n",
       " (no average)0.069 0.308 0.668 0.733 0.613 0.668 0.636\n",
       " Generating images from MEG.,\n",
       " While framing decoding as a retrieval task yields promising\n",
       " results, it requires the true image to be in the retrieval set – a well-posed problem which presents\n",
       " limited use-cases in practice.,\n",
       " To address this issue, we trained three distinct brain modules to predict\n",
       " the three embeddings that we use (see Section 2.5) to generate images (Fig. 5).,\n",
       " As confirmed by the\n",
       " evaluation metrics of Table 1, the generated images look relatively good, with multiple generated\n",
       " images sharing the correct ground-truth category.,\n",
       " However, they appear to contain limited low-level\n",
       " information about the true image.,\n",
       " Figure 5: Examples of generated images conditioned on MEG-based latent predictions.,\n",
       " The groups\n",
       " of three stacked rows represent best, average and worst generations, as evaluated by the sum of\n",
       " (minus) SwA V and SSIM.,\n",
       " The application of a very similar pipeline on an analogous fMRI dataset (Allen et al., 2022; Ozcelik\n",
       " & VanRullen, 2023) – using a simple Ridge regression – shows image reconstructions that share\n",
       " both high-level and low-level features with the true image Fig. S3).,\n",
       " Together, these results suggest\n",
       " that it is not the reconstruction pipeline which fails to reconstruct low-level features, but rather the\n",
       " MEG signals which contain little information at that level.,\n",
       " 8 4 D ISCUSSION\n",
       " Related work.,\n",
       " The present study shares several elements with previous MEG and electroen-\n",
       " cephalography (EEG) studies designed not to maximize decoding performance but to understand\n",
       " the cascade of visual processes in the brain.,\n",
       " In particular, previous studies have trained linear mod-\n",
       " els to either (1) classify a small set of images from brain activity (Grootswagers et al., 2019; King\n",
       " & Wyart, 2021), (2) predict brain activity from the latent representations of the images (Cichy et al.,\n",
       " 2017) or (3) quantify the similarity between these two modalities with representational similarity\n",
       " analysis (RSA) (Cichy et al., 2017; Bankson et al., 2018; Grootswagers et al., 2019; Gifford et al.,\n",
       " 2022).,\n",
       " While these studies also make use of image embeddings, their linear decoders are limited to\n",
       " classifying a small set of object classes, or to distinguishing pairs of images.,\n",
       " In addition, several deep neural networks have been introduced to maximize the classification of\n",
       " speech (D ´efossez et al., 2022), mental load (Jiao et al., 2018) and images (Palazzo et al., 2020;\n",
       " McCartney et al., 2022; Bagchi & Bathula, 2022) from EEG recordings.,\n",
       " In particular, Palazzo et al.\n",
       " (2020) introduced a deep convolutional neural network to classify natural images from EEG signals.,\n",
       " However, the experimental protocol consisted of presenting all of the images of the same class within\n",
       " a single continuous block, which risks allowing the decoder to rely on autocorrelated noise, rather\n",
       " than informative brain activity patterns (Li et al., 2020).,\n",
       " In any case, these EEG studies focus on the\n",
       " categorization of a relatively small number of images classes.,\n",
       " In sum, there is, to our knowledge, no MEG decoding study that learns end-to-end to reliably gen-\n",
       " erate an open set of images.\n",
       " Impact.,\n",
       " The present work has both fundamental and practical impacts.,\n",
       " First, the ability to decode\n",
       " complex perceptual representations as a function of time promises to greatly facilitate our under-\n",
       " standing of the processes at stake during visual processing in the brain.,\n",
       " There is considerable work\n",
       " inspecting the nature and the timing of the representations built along the visual system.,\n",
       " However,\n",
       " these results can be challenging to interpret, especially for high-level features.,\n",
       " Generative decoding,\n",
       " on the contrary, provides concrete and, thus, interpretable predictions.,\n",
       " Second, the most obvious\n",
       " use-case of brain decoding technology is to assist patients whose brain lesions challenge communi-\n",
       " cation.,\n",
       " This use-case, however, requires real-time decoding, and thus limit the use of neuroimaging\n",
       " modalities with low temporal resolution such as fMRI.,\n",
       " The present effort thus paves the way to\n",
       " achieve this long-awaited goal.\n",
       " Limitations.,\n",
       " Our analyses highlight three main limitations to the decoding of images from MEG\n",
       " signals.,\n",
       " First, the decoding of high-level semantic features prevails over low-level features: in\n",
       " particular, the generated images preserve semantics ( e.g. object categories) much better than low-\n",
       " level features ( e.g. contours, shading).,\n",
       " This phenomenon is difficult to attribute to our pipeline:\n",
       " indeed, applying a similar procedure to 7T fMRI recordings achieves reasonably high reconstruction\n",
       " of low-level features (Fig. S3).,\n",
       " Rather, this result resonates with the fact that the spatial resolution\n",
       " of MEG ( ≈cm) is much lower than 7T fMRI’s ( ≈mm).,\n",
       " Second, the present approach directly\n",
       " depends on the pretraining of several models, and only learns end-to-end to align the MEG signals\n",
       " to these pretrained embeddings.,\n",
       " Our results show that this approach leads to better performance than\n",
       " classical computer vision features such as color histograms, fast-Fourier transforms and histogram\n",
       " of oriented gradients (HOG).,\n",
       " This is consistent with a recent MEG study by D ´efossez et al. (2022)\n",
       " which showed, in the context of speech decoding, that pretrained embeddings outperformed a fully\n",
       " end-to-end approach.,\n",
       " Nevertheless, it remains to be tested whether (1) fine-tuning the image and\n",
       " generation modules and (2) combining the different types of visual features could improve decoding\n",
       " performance.,\n",
       " Ethical implications.,\n",
       " While the decoding of brain activity promises to help a variety of brain-\n",
       " lesioned patients (Metzger et al., 2023; Moses et al., 2021; D ´efossez et al., 2022; Liu et al., 2023;\n",
       " Willett et al., 2023), the rapid advances of this technology raise several ethical considerations, and\n",
       " most notably, the necessity to preserve mental privacy.,\n",
       " Several empirical findings are relevant to this\n",
       " issue.,\n",
       " Firstly, the decoding performance obtained with non-invasive recordings is only high for per-\n",
       " ceptual tasks.,\n",
       " By contrast, decoding accuracy considerably diminishes when individuals are tasked\n",
       " to imagine representations (Horikawa & Kamitani, 2017; Tang et al., 2023).,\n",
       " Second, decoding per-\n",
       " formance seems to be severely compromised when participants are engaged in disruptive tasks, such\n",
       " 9 as counting backward (Tang et al., 2023).,\n",
       " In other words, the subjects’ consent is not only a legal but\n",
       " also and primarily a technical requirement for brain decoding.,\n",
       " To delve into these issues effectively,\n",
       " we endorse the open and peer-reviewed research standards.\n",
       " Conclusion.,\n",
       " Overall, these results provide an important step towards the decoding of the visual\n",
       " processes continuously unfolding in the human brain.,\n",
       " REFERENCES\n",
       " Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle,\n",
       " Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al.,\n",
       " A massive 7T fMRI dataset to\n",
       " bridge cognitive neuroscience and artificial intelligence.,\n",
       " Nature neuroscience , 25(1):116–126,\n",
       " 2022.,\n",
       " Subhranil Bagchi and Deepti R Bathula.,\n",
       " EEG-ConvTransformer for single-trial EEG-based visual\n",
       " stimulus classification.,\n",
       " Pattern Recognition , 129:108757, 2022.,\n",
       " Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,\n",
       " Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al.,\n",
       " Vector-based\n",
       " navigation using grid-like representations in artificial agents.,\n",
       " Nature , 557(7705):429–433, 2018.,\n",
       " B.B. Bankson, M.N. Hebart, I.I.A. Groen, and C.I. Baker.,\n",
       " The temporal evolution of conceptual\n",
       " object representations revealed through models of behavior, semantics and deep neural networks.,\n",
       " NeuroImage , 178:172–182, 2018.,\n",
       " ISSN 1053-8119.,\n",
       " doi: https://doi.org/10.1016/j.neuroimage.,\n",
       " 2018.05.037.,\n",
       " URL https://www.sciencedirect.com/science/article/pii/\n",
       " S1053811918304440 .,\n",
       " G. Bradski.,\n",
       " The OpenCV Library.,\n",
       " Dr. Dobb’s Journal of Software Tools , 2000.,\n",
       " Charlotte Caucheteux, Alexandre Gramfort, and Jean-R ´emi King.,\n",
       " Evidence of a predictive coding\n",
       " hierarchy in the human brain listening to speech.,\n",
       " Nature human behaviour , 7(3):430–441, 2023.,\n",
       " Radoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, and Aude Oliva.,\n",
       " Dynamics of scene\n",
       " representations in the human brain revealed by magnetoencephalography and deep neural net-\n",
       " works.,\n",
       " NeuroImage , 153:346–358, 2017.,\n",
       " Alexandre D ´efossez, Charlotte Caucheteux, J ´er´emy Rapin, Ori Kabeli, and Jean-R ´emi King.,\n",
       " De-\n",
       " coding speech from non-invasive brain recordings.,\n",
       " arXiv preprint arXiv:2208.12266 , 2022.,\n",
       " Matteo Ferrante, Tommaso Boccato, and Nicola Toschi.,\n",
       " Semantic brain decoding: from fMRI to\n",
       " conceptually similar image reconstruction of visual stimuli.,\n",
       " arXiv preprint arXiv:2212.06726 ,\n",
       " 2022.,\n",
       " Alessandro T Gifford, Kshitij Dwivedi, Gemma Roig, and Radoslaw M Cichy.,\n",
       " A large and rich\n",
       " EEG dataset for modeling human visual object recognition.,\n",
       " NeuroImage , 264:119754, 2022.,\n",
       " Tijl Grootswagers, Amanda K Robinson, and Thomas A Carlson.,\n",
       " The representational dynamics of\n",
       " visual objects in rapid serial visual processing streams.,\n",
       " NeuroImage , 188:668–679, 2019.,\n",
       " S´ebastien B Hausmann, Alessandro Marin Vargas, Alexander Mathis, and Mackenzie W Mathis.,\n",
       " Measuring and modeling the motor system with machine learning.,\n",
       " Current opinion in neurobiol-\n",
       " ogy, 70:11–23, 2021.,\n",
       " Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wick-\n",
       " lin, and Chris I Baker.,\n",
       " THINGS:,\n",
       " A database of 1,854 object concepts and more than 26,000\n",
       " naturalistic object images.,\n",
       " PloS one , 14(10):e0223792, 2019.,\n",
       " Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis\n",
       " Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker.,\n",
       " THINGS-data, a multi-\n",
       " modal collection of large-scale datasets for investigating object representations in human brain\n",
       " and behavior.,\n",
       " eLife , 12:e82580, feb 2023.,\n",
       " ISSN 2050-084X. doi: 10.7554/eLife.82580.,\n",
       " URL\n",
       " https://doi.org/10.7554/eLife.82580 .,\n",
       " 10 Tomoyasu Horikawa and Yukiyasu Kamitani.,\n",
       " Generic decoding of seen and imagined objects using\n",
       " hierarchical visual features.,\n",
       " Nature communications , 8(1):15037, 2017.,\n",
       " David H Hubel and Torsten N Wiesel.,\n",
       " Receptive fields, binocular interaction and functional archi-\n",
       " tecture in the cat’s visual cortex.,\n",
       " The Journal of physiology , 160(1):106, 1962.,\n",
       " Vinay Jayaram and Alexandre Barachant.,\n",
       " MOABB: trustworthy algorithm benchmarking for bcis.,\n",
       " Journal of neural engineering , 15(6):066011, 2018.,\n",
       " Zhicheng Jiao, Xinbo Gao, Ying Wang, Jie Li, and Haojun Xu.,\n",
       " Deep convolutional neural networks\n",
       " for mental load classification based on EEG data.,\n",
       " Pattern Recognition , 76:582–595, 2018.,\n",
       " Yukiyasu Kamitani and Frank Tong.,\n",
       " Decoding the visual and subjective contents of the human\n",
       " brain.,\n",
       " Nature neuroscience , 8(5):679–685, 2005.,\n",
       " Nancy Kanwisher, Josh McDermott, and Marvin M Chun.,\n",
       " The fusiform face area: a module in\n",
       " human extrastriate cortex specialized for face perception.,\n",
       " Journal of neuroscience , 17(11):4302–\n",
       " 4311, 1997.,\n",
       " Jean-R ´emi King and Valentin Wyart.,\n",
       " The human brain encodes a chronicle of visual events at each\n",
       " instant of time through the multiplexing of traveling waves.,\n",
       " Journal of Neuroscience , 41(34):\n",
       " 7224–7233, 2021.,\n",
       " Diederik P Kingma and Jimmy Ba.,\n",
       " Adam: A method for stochastic optimization.,\n",
       " arXiv preprint\n",
       " arXiv:1412.6980 , 2014.,\n",
       " Ren Li, Jared S Johansen, Hamad Ahmed, Thomas V Ilyevsky, Ronnie B Wilbur, Hari M Bharad-\n",
       " waj, and Jeffrey Mark Siskind.,\n",
       " The perils and pitfalls of block design for EEG classification\n",
       " experiments.,\n",
       " IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(1):316–333,\n",
       " 2020.,\n",
       " Yan Liu, Zehao Zhao, Minpeng Xu, Haiqing Yu, Yanming Zhu, Jie Zhang, Linghao Bu, Xiaoluo\n",
       " Zhang, Junfeng Lu, Yuanning Li, et al.,\n",
       " Decoding and synthesizing tonal language speech from\n",
       " brain activity.,\n",
       " Science Advances , 9(23):eadh0478, 2023.,\n",
       " Weijian Mai and Zhijun Zhang.,\n",
       " Unibrain:,\n",
       " Unify image reconstruction and captioning all in one\n",
       " diffusion model from human brain activity.,\n",
       " arXiv preprint arXiv:2308.07428 , 2023.,\n",
       " Ben McCartney, Barry Devereux, and Jesus Martinez-del Rincon.,\n",
       " A zero-shot deep metric learn-\n",
       " ing approach to brain–computer interfaces for image retrieval.,\n",
       " Knowledge-Based Systems , 246:\n",
       " 108556, 2022.,\n",
       " Johannes Mehrer, Courtney J Spoerer, Emer C Jones, Nikolaus Kriegeskorte, and Tim C Kietzmann.,\n",
       " An ecologically motivated image dataset for deep learning yields better models of human vision.,\n",
       " Proceedings of the National Academy of Sciences , 118(8):e2011417118, 2021.,\n",
       " Sean L Metzger, Kaylo T Littlejohn, Alexander B Silva, David A Moses, Margaret P Seaton, Ran\n",
       " Wang, Maximilian E Dougherty, Jessie R Liu, Peter Wu, Michael A Berger, et al.,\n",
       " A high-\n",
       " performance neuroprosthesis for speech decoding and avatar control.,\n",
       " Nature , pp.,\n",
       " 1–10, 2023.,\n",
       " David A Moses, Sean L Metzger, Jessie R Liu, Gopala K Anumanchipalli, Joseph G Makin,\n",
       " Pengfei F Sun, Josh Chartier, Maximilian E Dougherty, Patricia M Liu, Gary M Abrams, et al.\n",
       " Neuroprosthesis for decoding speech in a paralyzed person with anarthria.,\n",
       " New England Journal\n",
       " of Medicine , 385(3):217–227, 2021.,\n",
       " Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant.,\n",
       " Re-\n",
       " constructing visual experiences from brain activity evoked by natural movies.,\n",
       " Current biology ,\n",
       " 21(19):1641–1646, 2011.,\n",
       " John O’Keefe and Lynn Nadel.,\n",
       " The hippocampus as a cognitive map.,\n",
       " Behavioral and Brain Sci-\n",
       " ences , 2(4):487–494, 1979.,\n",
       " Furkan Ozcelik and Rufin VanRullen.,\n",
       " Brain-diffuser: Natural scene reconstruction from fMRI\n",
       " signals using generative latent diffusion.,\n",
       " arXiv preprint arXiv:2303.05334 , 2023.,\n",
       " 11 Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and\n",
       " Mubarak Shah.,\n",
       " Decoding brain representations by multimodal learning of neural activity and\n",
       " visual features.,\n",
       " IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(11):3833–\n",
       " 3849, 2020.,\n",
       " Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\n",
       " wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\n",
       " Sutskever.,\n",
       " Learning transferable visual models from natural language supervision, 2021.,\n",
       " Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Joce-\n",
       " lyn Faubert.,\n",
       " Deep learning-based electroencephalography analysis: a systematic review.,\n",
       " Journal\n",
       " of neural engineering , 16(5):051001, 2019.,\n",
       " Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\n",
       " Joshua Tenenbaum, and Evelina Fedorenko.,\n",
       " Artificial neural networks accurately predict lan-\n",
       " guage processing in the brain.,\n",
       " BioRxiv , pp.,\n",
       " 2020–06, 2020.,\n",
       " Paul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen,\n",
       " Aidan J Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, et al.,\n",
       " Reconstructing\n",
       " the mind’s eye: fMRI-to-image with contrastive learning and diffusion priors.,\n",
       " arXiv preprint\n",
       " arXiv:2305.18274 , 2023.,\n",
       " Katja Seeliger, Umut G ¨uc ¸l¨u, Luca Ambrogioni, Yagmur G ¨uc ¸l¨ut¨urk, and Marcel AJ van Gerven.,\n",
       " Generative adversarial networks for reconstructing natural images from brain activity.,\n",
       " NeuroIm-\n",
       " age, 181:775–785, 2018.,\n",
       " Yu Takagi and Shinji Nishimoto.,\n",
       " High-resolution image reconstruction with latent diffusion models\n",
       " from human brain activity.,\n",
       " bioRxiv , 2023.,\n",
       " doi: 10.1101/2022.11.18.517004.,\n",
       " URL https:\n",
       " //www.biorxiv.org/content/early/2023/03/11/2022.11.18.517004 .,\n",
       " Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth.,\n",
       " Semantic reconstruction of con-\n",
       " tinuous language from non-invasive brain recordings.,\n",
       " Nature Neuroscience , pp. 1–9, 2023.,\n",
       " Armin Thomas, Christopher R ´e, and Russell Poldrack.,\n",
       " Self-supervised learning of brain dynamics\n",
       " from broad neuroimaging data.,\n",
       " Advances in Neural Information Processing Systems , 35:21255–\n",
       " 21269, 2022.,\n",
       " Stefan Van der Walt, Johannes L Sch ¨onberger, Juan Nunez-Iglesias, Franc ¸ois Boulogne, Joshua D\n",
       " Warner, Neil Yager, Emmanuelle Gouillart, and Tony Yu.,\n",
       " scikit-image: image processing in\n",
       " python.,\n",
       " PeerJ , 2:e453, 2014.,\n",
       " Rufin VanRullen and Leila Reddy.,\n",
       " Reconstructing faces from fMRI patterns using deep generative\n",
       " neural networks.,\n",
       " Communications biology , 2(1):193, 2019.,\n",
       " Francis R Willett, Erin M Kunz, Chaofei Fan, Donald T Avansino, Guy H Wilson, Eun Young\n",
       " Choi, Foram Kamdar, Matthew F Glasser, Leigh R Hochberg, Shaul Druckmann, et al.,\n",
       " A high-\n",
       " performance speech neuroprosthesis.,\n",
       " Nature , pp. 1–6, 2023.,\n",
       " Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J\n",
       " DiCarlo.,\n",
       " Performance-optimized hierarchical models predict neural responses in higher visual\n",
       " cortex.,\n",
       " Proceedings of the national academy of sciences , 111(23):8619–8624, 2014.,\n",
       " Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang\n",
       " Liu, and Baochang Zhang.,\n",
       " Controllable mind visual diffusion model.,\n",
       " arXiv preprint\n",
       " arXiv:2305.10135 , 2023.,\n",
       " 12,\n",
       " A A PPENDIX\n",
       " A.1 L INEAR RIDGE REGRESSION SCORES ON PRETRAINED IMAGE REPRESENTATIONS\n",
       " We provide a (5-fold cross-validated) Ridge regression baseline (Table S1) for comparison with our\n",
       " brain module results of Section 3, showing considerable improvements for the latter.,\n",
       " Table S1:,\n",
       " Image retrieval performance of a linear Ridge regression baseline on pretrained image\n",
       " representations\n",
       " Top-5 acc (%) ↑,\n",
       " Median relative rank ↓\n",
       " Latent kind Latent name Small set Large set Small set Large set\n",
       " Text/Image\n",
       " alignmentCLIP-Vision (CLS) 10.5 0.50 0.23,\n",
       " 0.34\n",
       " CLIP-Text (mean) 6.0 0.25 0.42 0.43\n",
       " CLIP-Vision (mean) 5.5 0.46 0.32 0.37\n",
       " Feature\n",
       " engineeringColor histogram 7.0 0.33 0.31 0.40\n",
       " Local binary patterns (LBP) 3.5 0.37 0.34 0.44\n",
       " FFT 2D (as real) 4.5 0.46 0.40 0.45\n",
       " HOG 3.0 0.42 0.45 0.46\n",
       " FFT 2D (log-PSD and angle) 2.0 0.37 0.47 0.46\n",
       " Variational\n",
       " autoencoderAutoKL 7.5 0.54 0.24 0.38\n",
       " VDV AE 8.0 0.50 0.33 0.43\n",
       " Self-supervised\n",
       " learning DINOv2,\n",
       " (CLS) 7.5 0.46 0.25 0.35\n",
       " SupervisedVGG-19 12.5 1.04 0.18 0.33\n",
       " ResNet-101 4.0 0.37 0.36 0.42\n",
       " DenseNet-201 5.0 0.29 0.39 0.45\n",
       " Wide ResNet-101-2 3.5 0.42 0.40 0.46\n",
       " MobileNet v3 3.5 0.42 0.40 0.42\n",
       " A.2 H YPERPARAMETER SEARCH,\n",
       " We run a hyperparameter search to find an appropriate configuration (MEG preprocessing, opti-\n",
       " mizer, brain module architecture and loss definition) for the MEG-to-image retrieval task ( λ= 0).,\n",
       " We randomly split the 79,392 (MEG, image) pairs of the adapted training set (Section 2.8) into\n",
       " 60%-20%-20% train, valid and test splits such that all presentations of a given image are contained\n",
       " in the same split.,\n",
       " We use the validation split to perform early stopping and the test split to evaluate\n",
       " the performance of a configuration.,\n",
       " For the purpose of this search we pick CLIP-Vision (CLS) latent as a representative latent, since\n",
       " it achieved good retrieval performance in preliminary experiments.,\n",
       " We run the search six times\n",
       " using two different random seed initializations for the brain module and three different random\n",
       " train/valid/test splits.,\n",
       " Fig.,\n",
       " S1 summarizes the results of this hyperparameter search.,\n",
       " Based on this search, we use the following configuration: MEG window (tmin, tmax)of\n",
       " [−0.5,1.0]s, learning rate of 3×10−4, batch size of 128, brain module with two convolutional\n",
       " blocks and both the spatial attention and subject layers of D ´efossez et al. (2022), affine projection\n",
       " temporal aggregation layer with a single block in the CLIP projection head, and full CLIP loss (in-\n",
       " cluding learned temperature parameter, normalization along both axes and symmetric terms).,\n",
       " The\n",
       " final architecture configuration is presented in Table S2.,\n",
       " A.3 F ULL-WINDOW MEG- BASED IMAGE RETRIEVALS\n",
       " Fig.,\n",
       " S2 shows examples of retrieved images based on the best performing latents identified in Sec-\n",
       " tion 3.\n",
       " 13 Figure S1:,\n",
       " Hyperparameter search results for the MEG-to-image retrieval task, presenting the impact\n",
       " of (A) optimizer learning rate and batch size, ( B) number of convolutional blocks and use of spatial\n",
       " attention and/or subject-specific layers in the brain module, ( C) MEG window parameters, ( D) type\n",
       " of temporal aggregation layer and number of blocks in the CLIP projection head of the brain module,\n",
       " and ( E) CLIP loss configuration (normalization axes, use of learned temperature parameter and use\n",
       " of symmetric terms).,\n",
       " Chance-level performance top-5 accuracy is 0.05%.,\n",
       " Table S2: Brain module configuration adapted from D ´efossez et al. (2022) for use with a target latent\n",
       " of size 768 ( e.g. CLIP-Vision (CLS), see Section 2.4) in retrieval settings.,\n",
       " Layer Input shape Output shape # parameters\n",
       " Spatial attention block (272, 181) (270, 181) 552,960\n",
       " Linear projection (270, 181) (270, 181) 73,170\n",
       " Subject-specific linear layer (270, 181) (270, 181) 291,600\n",
       " Residual dilated conv block 1 (270, 181) (320, 181) 1,183,360\n",
       " Residual dilated conv block 2 (320, 181) (320, 181) 1,231,360\n",
       " Linear projection (320, 181) (2048, 181) 1,518,208\n",
       " Temporal aggregation (2048, 181) (2048, 1) 182\n",
       " MLP projector (2048, 1) (768, 1) 1,573,632\n",
       " Total 6,424,472\n",
       " 14 Figure S2: Representative examples of retrievals (top-4) using models trained on full windows (from\n",
       " -0.5 s to 1 s after image onset).,\n",
       " Retrieval set: N= 6,059images from 1,196categories.,\n",
       " A.4 I MAGE EMBEDDINGS\n",
       " We evaluate the performance of linear baselines and of a deep convolutional neural network on the\n",
       " MEG-to-image retrieval task using a set of classic visual embeddings.,\n",
       " We grouped these embeddings\n",
       " by their corresponding paradigm:\n",
       " Supervised learning.,\n",
       " DenseNet-121,DenseNet-169,DenseNet-201, MobileNet v2, MobileNet\n",
       " v3, ResNet-101, ResNet-18, ResNet-50, ResNext101-32-8d, ResNext50-32-4d, VGG-16,VGG-19,\n",
       " Wide ResNet-101-2, Wide ResNet-50-2.\n",
       " Text/Image alignment.,\n",
       " CLIP-Vision, CLIP-Text, and their CLS and MEAN pooling.,\n",
       " Self-supervised learning.,\n",
       " DINOv1, DINOv2 and their CLS and MEAN pooling.,\n",
       " 15 Figure S3:,\n",
       " Examples of generated images conditioned on fMRI-based latent predictions.,\n",
       " The groups\n",
       " of three stacked rows represent best, average and worst retrievals, as evaluated by the sum of (minus)\n",
       " SwA V and SSIM.,\n",
       " Variational autoencoders.,\n",
       " The activations of the 31 first layers of the very deep variational-\n",
       " autoencoder (VDV AE), and the Kullback-Leibler variational-autoencoder (AutoKL) used in the gen-\n",
       " erative module (Section 2.5).,\n",
       " Engineered features.,\n",
       " The color histogram of the seen image (8 bins per channels); the local binary\n",
       " patterns (LBP) using the implementation in OpenCV 2 (Bradski, 2000) with ’uniform’ method,\n",
       " P= 8 andR= 1; the Histogram of Oriented Gradients (HOG) using the implementation of sk-\n",
       " image (Van der Walt et al., 2014) with 8 orientations, 8 pixels-per-cell and 2 cells-per-block.,\n",
       " A.5 7T FMRI DATASET\n",
       " The Natural Scenes Dataset (NSD) (Allen et al., 2022) contains fMRI data from 8 participants\n",
       " viewing a total of 73,000 RGB images.,\n",
       " It has been successfully used for reconstructing seen images\n",
       " from fMRI in several studies (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023; Scotti et al.,\n",
       " 2023).,\n",
       " In particular, these studies use a highly preprocessed, compact version of fMRI data (“betas”)\n",
       " obtained through generalized linear models fitted across multiple repetitions of the same image.,\n",
       " Each participant saw a total of 10,000 unique images (repeated 3 times each) across 37 sessions.,\n",
       " Each session consisted in 12 runs of 5 minutes each, where each image was seen during 3 s, with\n",
       " a 1-s blank interval between two successive image presentations.,\n",
       " Among the 8 participants, only 4\n",
       " (namely 1, 2, 5 and 7) completed all sessions.,\n",
       " To compute the three latents used to reconstruct the seen images from fMRI data (as described in\n",
       " Section 2.5) we follow Ozcelik & VanRullen (2023) and train and evaluate three distinct Ridge\n",
       " regression models using the exact same split.,\n",
       " That is, for each of the four remaining participants,\n",
       " the 9,000 uniquely-seen-per-participant images (and their three repetitions) are used for training,\n",
       " and a common set of 1000 images seen by all participant is kept for evaluation (also with their three\n",
       " repetitions).,\n",
       " We report reconstructions and metrics for participant 1.\n",
       " Theαcoefficient for the L2-regularization of the regressions are cross-validated with a 5-fold\n",
       " scheme on the training set of each subject.,\n",
       " We follow the same standardization scheme for inputs\n",
       " and predictions as in (Ozcelik & VanRullen, 2023).,\n",
       " Fig.,\n",
       " S3 presents generated images obtained using the NSD dataset (Allen et al., 2022).,\n",
       " 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Chunk 1: \n",
      "\n",
      "mbeddings obtained from the image, ii) an MEG\n",
      "\n",
      "=====\n",
      "\n",
      "### Chunk 2: \n",
      "\n",
      "module trained end-to-end and iii) a pretrained image generator. Our results are\n",
      "\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Initialize the text splitter with custom parameters\n",
    "custom_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set custom chunk size\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    # Use length of the text as the size measure\n",
    "    length_function = len,\n",
    "    separators=['\\n']\n",
    "\n",
    ")\n",
    "\n",
    "# Create the chunks\n",
    "texts = custom_text_splitter.create_documents([sample])\n",
    "\n",
    "# Print the first two chunks\n",
    "print(f'### Chunk 1: \\n\\n{texts[0].page_content}\\n\\n=====\\n')\n",
    "print(f'### Chunk 2: \\n\\n{texts[1].page_content}\\n\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRAIN DECODING :TOWARD REAL -TIME\\nRECONSTRUCTION OF VISUAL PERCEPTION\\nYohann Benchetrit1,∗, Hubert Banville1,∗, Jean-R ´emi King1,2\\n1FAIR, Meta,2Laboratoire des Syst `emes Perceptifs, ´Ecole Normale Sup ´erieure, PSL University\\n{ybenchetrit,hubertjb,jeanremi }@meta.com\\nABSTRACT\\nIn the past five years, the use of generative and foundational AI systems has\\ngreatly improved the decoding of brain activity. Visual perception, in particular,\\ncan now be decoded from functional Magnetic Resonance Imaging (fMRI) with\\nremarkable fidelity. This neuroimaging technique, however, suffers from a lim-\\nited temporal resolution ( ≈0.5 Hz) and thus fundamentally constrains its real-time\\nusage. Here, we propose an alternative approach based on magnetoencephalog-\\nraphy (MEG), a neuroimaging device capable of measuring brain activity with\\nhigh temporal resolution ( ≈5,000 Hz). For this, we develop an MEG decoding\\nmodel trained with both contrastive and regression objectives and consisting of\\nthree modules: i) pretrained embeddings obtained from the image, ii) an MEG\\nmodule trained end-to-end and iii) a pretrained image generator. Our results are\\nthreefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval\\nover classic linear decoders. Second, late brain responses to images are best de-\\ncoded with DINOv2, a recent foundational image model. Third, image retrievals\\nand generations both suggest that MEG signals primarily contain high-level visual\\nfeatures, whereas the same approach applied to 7T fMRI also recovers low-level\\nfeatures. Overall, these results provide an important step towards the decoding\\n– in real time – of the visual processes continuously unfolding within the human\\nbrain.\\n 1 I NTRODUCTION\\nAutomating the discovery of brain representations. Understanding how the human brain rep-\\nresents the world is arguably one of the most profound scientific challenges.',\n",
       " 'This quest, which\\noriginally consisted of searching, one by one, for the specific features that trigger each neuron, ( e.g.\\nHubel & Wiesel (1962); O’Keefe & Nadel (1979); Kanwisher et al. (1997)), is now being automated\\nby Machine Learning (ML) in two mains ways. First, as a signal processing tool, ML algorithms are\\ntrained to extract informative patterns of brain activity in a data-driven manner. For example, Kami-\\ntani & Tong (2005) trained a support vector machine to classify the orientations of visual gratings\\nfrom functional Magnetic Resonance Imaging (fMRI). Since then, deep learning has been increas-\\ningly used to discover such brain activity patterns (Roy et al., 2019; Thomas et al., 2022; Jayaram\\n& Barachant, 2018; D ´efossez et al., 2022; Scotti et al., 2023).',\n",
       " 'Second, ML algorithms are used as\\nfunctional models of the brain. For example, Yamins et al. (2014) have shown that the embedding\\nof natural images in pretrained deep nets linearly account for the neuronal responses to these images\\nin the cortex. Since, pretrained deep learning models have been shown to account for a wide variety\\nof stimuli including text, speech, navigation, and motor movement (Banino et al., 2018; Schrimpf\\net al., 2020; Hausmann et al., 2021; Mehrer et al., 2021; Caucheteux et al., 2023).\\n',\n",
       " 'Generating images from brain activity. This observed representational alignment between brain\\nactivity and deep learning models creates a new opportunity: Decoding of visual stimuli need not\\nbe restricted to a limited set of classes, but can now leverage pretrained representations to condi-\\ntion subsequent generative AI models. While the resulting image may be partly “hallucinated”,\\ninterpreting images can be much simpler than interpreting latent features.',\n",
       " 'Following a long series\\n∗Equal contribution.\\n1 of generative approaches (Nishimoto et al., 2011; Kamitani & Tong, 2005; VanRullen & Reddy,\\n2019; Seeliger et al., 2018), diffusion techniques have, in this regard, significantly improved the\\ngeneration of images from functional Magnetic Resonance Imaging (fMRI). The resulting pipeline\\ntypically consists of three main modules: (1) a set of pretrained embeddings obtained from the im-\\nage onto which (2) fMRI activity can be linearly mapped and (3) ultimately used to condition a\\npretrained image-generation model (Ozcelik & VanRullen, 2023; Mai & Zhang, 2023; Zeng et al.,\\n2023; Ferrante et al., 2022). These recent fMRI studies primarily differ in the type of pretrained\\nimage-generation model that they use.\\n The challenge of real-time decoding. This generative decoding approach has been mainly applied\\nto fMRI.',\n",
       " 'However, the temporal resolution of fMRI is limited by the time scale of blood flow and\\ntypically leads to one snapshot of brain activity every two seconds – a time scale that challenges its\\nclinical usage, e.g. for patients who require a brain-computer-interface (Willett et al., 2023; Moses\\net al., 2021; Metzger et al., 2023; D ´efossez et al., 2022). On the contrary, magnetoencephalography\\n(MEG) can measure brain activity at a much higher temporal resolution ( ≈5,000 Hz) by recording\\nthe fluctuation of magnetic fields elicited by the post-synaptic potentials of pyramidal neurons. This\\nhigher temporal resolution comes at cost, however: the spatial resolution of MEG is limited to ≈300\\nsensors, whereas fMRI measures ≈100,000 voxels. In sum, fMRI intrinsically limits our ability to\\n(1) track the dynamics of neuronal activity, (2) decode dynamic stimuli (speech, videos etc) and\\n(3) apply these tools to real-time use cases. Conversely, it is unknown whether temporally-resolved\\nneuroimaging systems like MEG are sufficiently precise to generate natural images in real-time.\\n',\n",
       " 'Combining previous work on speech retrieval from MEG (D ´efossez et al., 2022)\\nand on image generation from fMRI (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023),\\nwe here develop a three-module pipeline trained to (1) align MEG activity onto pretrained visual\\nembeddings and (2) generate images from a stream of MEG signals (Fig. 1).\\n Figure 1: ( A) Approach.',\n",
       " 'Unlike image\\ngeneration, image retrieval can be done in the aligned latent space, but requires the true image in the\\nretrieval set.\\n Our systematic benchmark provides two main contributions: our MEG decoder leads to (1) high-\\nperforming image retrieval and image generation, (2) new means to interpret the unfolding of visual\\nprocessing in the brain. This demonstrates the capacity of our approach to truly generalize to new\\nvisual concepts, paving the way to “free-form” visual decoding. Overall, our findings outline a\\npromising avenue for real-time decoding of visual representations in the lab and in the clinic.\\n',\n",
       " 'We aim to decode images from multivariate time series of brain activity recorded with MEG as\\nhealthy participants watched a sequence of natural images. Let Xi∈RC×Tbe the MEG time\\nwindow collected as an image Iiwas presented to the participant, where Cis the number of MEG\\nchannels, Tis the number of time points in the MEG window and i∈[ [1, N] ]. Letzi∈RFbe the\\nlatent representation of Ii, with Fthe number of features, obtained by embedding the image using\\na pretrained image model (Section 2.4). As described in more detail below, our decoding approach\\nrelies on training a brain module fθ:RC×T→RFto maximally retrieve or predict Iithrough zi,\\ngiven Xi.\\n2.2 T RAINING OBJECTIVES\\n',\n",
       " 'We use different training objectives for the different parts of our proposed pipeline. First, in the case\\nof retrieval, we aim to pick the right image Ii(i.e., the one corresponding to Xi) out of a bank of\\ncandidate images. To do so, we train fθusing the CLIP loss (Radford et al., 2021) on batches of size\\nB with exactly one positive example:\\nLCLIP (θ) =−1\\nBBX\\ni=1 \\nlogexp(s(ˆzi,zi)/τ)PB\\nj=1exp(s(ˆzi,zj)/τ)+ logexp(s(ˆzi,zi)/τ)PB\\nk=1exp(s(ˆzk,zi)/τ)!\\n (1)\\nwhere sis the cosine similarity, ziandˆzi=fθ(Xi)are the latent representation and the correspond-\\ning MEG-based prediction, respectively, and τis a learned temperature parameter.\\n',\n",
       " 'Next, to go beyond retrieval and instead generate images, we train fθto directly predict the latent\\nrepresentations zsuch that we can use them to condition generative image models.',\n",
       " 'This is done\\nusing a standard mean squared error (MSE) loss:\\nLMSE(θ) =1\\nNFNX\\ni=1∥zi−ˆzi∥2\\n2 (2)\\n Finally, we combine the CLIP and MSE losses using a convex combination with tuned weight to\\ntrain models that benefit from both training objectives:\\nLCombined =λLCLIP + (1−λ)LMSE (3)\\n2.3 B RAIN MODULE\\n We adapt the dilated residual ConvNet architecture of D ´efossez et al. (2022), denoted as fθ, to\\nlearn the projection from an MEG window Xi∈RC×Tto a latent image representation zi∈RF.\\n The original model’s output ˆYbackbone ∈RF′×Tmaintains the temporal dimension of the network\\nthrough its residual blocks. However, here we regress a single latent per input instead of a sequence\\nofTlatents like in D ´efossez et al. (2022). Consequently, we add a temporal aggregation layer\\nto reduce the temporal dimension of ˆYbackbone to obtain ˆyagg∈RF′. We experiment with three\\ntypes of aggregations: global average pooling, a learned affine projection, and an attention layer.\\n Finally, we add two MLP heads1,i.e., one for each term in LCombined , to project from F′to the F\\ndimensions of the target latent.\\n We run a hyperparameter search to identify an appropriate configuration of preprocessing, brain\\nmodule architecture, optimizer and loss hyperparameters for the retrieval task (see Appendix A.2).\\n The final architecture configuration for retrieval is described in Table S2 and contains e.g. 6.4M\\ntrainable parameters for F= 768 .\\n 1A head consists of repeated LayerNorm-GELU-Linear blocks.\\n',\n",
       " 'For image generation experiments, the output of the MSE head is further postprocessed as in Ozcelik\\n& VanRullen (2023), i.e., we z-score normalize each feature across predictions, and then apply the\\ninverse z-score transform fitted on the training set (defined by the mean and standard deviation of\\neach feature dimension on the target embeddings). We select λinLCombined by sweeping over\\n{0.0,0.25,0.5,0.75,1.0}and pick the model whose top-5 accuracy is the highest on the large test\\nset. Of note, when training models to generate CLIP and AutoKL latents, we simplify the task of\\nthe CLIP head by reducing the dimensionality of its target: we use the CLS token for CLIP-Vision\\n(FMSE = 768 ), the ”mean” token for CLIP-Text ( FMSE = 768 ), and the channel-average for\\nAutoKL latents ( FMSE = 4096 ), respectively.\\n 2.4 I MAGE MODULES\\n',\n",
       " 'We study the functional alignment between brain activity and a variety of (output) embeddings\\nobtained from deep neural networks trained in three different representation learning paradigms,\\nspanning a wide range of dimensionalities: supervised learning ( e.g. VGG-19), image-text align-\\nment (CLIP), and variational autoencoders. When using vision transformers, we further include two\\nadditional embeddings of smaller dimensionality: the average of all output embeddings across to-\\nkens (mean), and the output embedding of the class-token (CLS). For comparison, we also evaluate\\nour approach on human-engineered features obtained without deep learning. The list of embeddings\\nis provided in Appendix A.4. For clarity, we focus our experiments on a representative subset.\\n 2.5 G ENERATION MODULE\\nTo fairly compare our work to the results obtained with fMRI results, we follow the approach of\\nOzcelik & VanRullen (2023) and use a model trained to generate images from pretrained embed-\\ndings. Specifically, we use a latent diffusion model conditioned on three embeddings: CLIP-Vision\\n(257tokens ×768), CLIP-Text ( 77tokens ×768), and a variational autoencoder latent (AutoKL;\\n(4×64×64). Following Ozcelik & VanRullen (2023), we apply diffusion with 50 DDIM steps, a\\nguidance of 7.5, a strength of 0.75 with respect to the image-to-image pipeline, and a mixing of 0.4.\\n2.6 T RAINING AND COMPUTATIONAL CONSIDERATIONS\\nCross-participant models are trained on a set of ≈63,000 examples using the Adam optimizer\\n(Kingma & Ba, 2014) with learning rate of 3×10−4and a batch size of 128. We use early stopping\\non a validation set of ≈15,800examples randomly sampled from the original training set, with a\\npatience of 10, and evaluate the performance of the model on a held-out test set (see below). Models\\nare trained on a single V olta GPU with 32 GB of memory. We train each model three times using\\nthree different random seeds for the weight initialization of the brain module.\\n 2.7 E VALUATION\\nRetrieval metrics. We first evaluate decoding performance using retrieval metrics. For a known\\ntest set, we are interested in the probability of identifying the correct image given the model predic-\\ntions. Retrieval metrics have the advantage of sharing the same scale regardless of the dimensional-\\nity of the MEG (like encoding metrics), the dimensionality of the image embedding (like regression\\nmetrics). We evaluate retrieval using either the relative median rank (which does not depend on the\\nsize of the retrieval set), defined as the rank of a prediction divided by the size of the retrieval set, or\\nthetop-5 accuracy (which is more common in the literature).\\n',\n",
       " 'Decoding performance is often measured qualitatively as well as quantita-\\ntively using a variety of metrics reflecting the reconstruction fidelity both in terms of perception and\\nsemantics. For fair comparison with fMRI generations, we provide the same metrics as Ozcelik &\\nVanRullen (2023), computed between seen and generated images: PixCorr (the pixel-wise correla-\\ntion between the true and generated images), SSIM (Structural Similarity Index Metric), and SwA V\\n(the correlation with respect to SwA V-ResNet50 output). On the other hand, AlexNet(2/5), Incep-\\ntion, and CLIP are the respective 2-way comparison scores of layers 2/5 of AlexNet, the pooled last\\nlayer of Inception and the output layer of CLIP. For the NSD dataset, these metrics are reported for\\nparticipant 1 only (see Appendix A.5).\\n 4 To avoid non-representative cherry-picking, we sort all generations on the test set according to the\\nsum of (minus) SwA V and SSIM. We then split the data into 15 blocks and pick 4 images from the\\nbest, middle and worst blocks with respect to the summed metric.\\n Real-time and average metrics.',\n",
       " 'It is common in fMRI to decode brain activity from preprocessed\\nvalues estimated with a General Linear Model. These “beta values” are estimates of brain responses\\nto individual images, computed across multiple repetitions of such images. To provide a fair as-\\nsessment of possible MEG decoding performance, we thus leverage repeated image presentations\\navailable in the datasets (see below) by averaging predictions before evaluating metrics.\\n 2.8 D ATASET\\nWe test our approach on the “THINGS-MEG” dataset (Hebart et al., 2023). Four participants (2\\nfemales, 2 males; mean age of 23.25 years), underwent 12 MEG sessions during which they were\\npresented with a set of 22,448 unique images selected from the THINGS database (Hebart et al.,\\n2019), covering 1,854 categories. Of those, only a subset of 200 images (each one of a different cat-\\negory) was shown multiple times to the participants. The images were displayed for 500 ms each,\\nwith a variable fixation period of 1000±200ms between presentations. The THINGS dataset ad-\\nditionally contains 3,659 images that were not shown to the participants and that we use to augment\\nthe size of our retrieval set and emphasize the robustness of our method.\\n',\n",
       " 'MEG Preprocessing. We use a minimal MEG data-preprocessing pipeline as in D ´efossez et al.\\n(2022). Raw data from the 272 MEG radial gradiometer channels is downsampled from 1,200 Hz to\\n120 Hz before being centered and clipped channel-wise above +/- 5 standard errors. The continuous\\nMEG data is then epoched from -500 ms to 1,000 ms relative to stimulus onset. Finally, baseline-\\ncorrection is performed by subtracting the mean signal value observed between the start of an epoch\\nand the stimulus onset for each channel.\\n',\n",
       " 'The original split of Hebart et al. (2023) consists of 22,248 uniquely presented images,\\nand 200 test images repeated 12 times each for each participant ( i.e., 2,400 trials per participant).\\n The use of this data split presents a challenge, however, as the test set contains only one image\\nper category, and these categories are also seen in the training set. This means evaluating retrieval\\nperformance on this test set does not measure the capacity of the model to (1) extrapolate to new\\nunseen categories of images and (2) recover a particular image within a set of multiple images of\\nthe same category, but rather only to “categorize” it. Consequently, we propose two modifications\\nof the original split. First, we remove from the training set any image whose category appears in the\\noriginal test set. This “adapted training set” removes any categorical leakage across the train/test\\nsplit and makes it possible to assess the capacity of the model to decode images of unseen image\\ncategories ( i.e., a “zero-shot” setting). Second, we propose a new “large test set” that is built using\\nthe images removed from the training set. This new test set effectively allows evaluating retrieval\\nperformance of images within images of the same category2. We report results on both the original\\n(“small”) and the “large” test sets to enable comparisons with the original settings of Ozcelik &\\nVanRullen (2023). Finally, we also compare our results to the performance obtained by a similar\\npipeline but trained on fMRI data using the NSD dataset (Allen et al., 2022) (see Appendix A.5).\\n 3 R ESULTS\\nML as an effective model of the brain.',\n",
       " 'Which representations of natural images are likely to\\nmaximize decoding performance? To answer this question, we compare the retrieval performance\\nobtained by linear Ridge regression models trained to predict one of 16 different latent visual rep-\\nresentations given the flattened MEG response Xito each image Ii(Table S1). While all image\\nembeddings lead to above-chance retrieval, supervised and text/image alignment models ( e.g. VGG,\\nCLIP) yield the highest retrieval scores.\\n 2We leave out images of the original test set from this new large test set, as keeping them would create a\\ndiscrepancy between the number of MEG repetitions for training images and test images.\\n 5 ML as an effective toolto learn brain responses. We then compare these linear baselines to\\na deep ConvNet architecture (D ´efossez et al., 2022) trained on the same task3,i.e., to retrieve the\\nmatching image given an MEG window. Using a deep model leads to a 7X improvement over the lin-\\near baselines (Fig. 2). Multiple types of image embeddings lead to good retrieval performance, with\\nVGG-19 (supervised learning), CLIP-Vision (text/image alignment) and DINOv2 (self-supervised\\nlearning) yielding top-5 accuracies of 70.33±2.80%,68.66±2.84%,68.00±2.86%, respectively\\n(where the standard error of the mean is computed across the averaged image-wise metrics). Similar\\nconclusions, although with lower performance, can be drawn from our “large” test set setting, where\\ndecoding cannot rely solely on the image category but also requires discriminating between multiple\\nimages of the same category. Representative retrieval examples are shown in Appendix A.3.\\nFigure 2: Image retrieval performance obtained from a trained deep ConvNet. The original “small”\\ntest set (Hebart et al., 2023) comprises 200 distinct images, each belonging to a different category.\\n In contrast, our proposed “large” test set comprises 12 images from each of those 200 categories,\\nyielding a total of 2,400 images. Chance-level is 2.5% top-5 accuracy for the small test set and\\n0.21% for the large test set. The best latent representations yield accuracies around 70% and 13%\\nfor the small and large test sets, respectively.\\n',\n",
       " 'The above results are obtained from the full time window\\n(-500 ms to 1,000 ms relative to stimulus onset). To further investigate the possibility of decoding\\nvisual representations as they unfold in the brain, we repeat this analysis on 250 ms-long sliding win-\\ndows (Fig. 3). For clarity, we focus on a subset of representative image embeddings. As expected,\\nall models yield chance-level performance before the image presentation. For all models, a first\\nclear peak can then be observed on the 0 to 250-ms window, followed by a second peak, after the\\nimage offset, which then quickly goes back to chance-level. Interestingly, the recent self-supervised\\nmodel DINOv2 yields particularly good retrieval performance after the image offset.\\n To get a better sense of what the above decoding metrics mean, we present the top-1 retrieved images\\nfrom an augmented retrieval set built by concatenating the “large” test set with an additional set of\\n3,659 images that were not seen by the participants (Fig. 4).\\n Overall, the retrieved images tend to come from the correct category, such as “speaker” or “brocoli”,\\nmostly during the first few sub-windows ( t≤1s). However, these retrieved images do not appear\\nto share obvious low-level features to the images seen by the participants.\\n 3We use λ= 1inLCombined as we are solely concerned with the retrieval part of the pipeline here.\\n 6 Figure 3: Retrieval performance of models trained on 250 ms sliding windows for different image\\nembeddings. The shaded gray area indicates the 0.5-s interval during which the image was presented\\nto the participants.',\n",
       " 'Accuracy generally peaked right after the image onset and offset.\\n',\n",
       " 'Figure 4: Representative examples of dynamic retrievals using CLIP-Vision (CLS) and models\\ntrained on 250-ms sliding windows (Image onset: t= 0, retrieval set: N= 6,059from 1,196\\ncategories). The groups of three stacked rows represent best, average and worst retrievals, obtained\\nby sampling examples from the <10%, 45-55% and >90% percentile groups based on top-5 accu-\\nracy.\\n Overall, and while further analyses of these results remain necessary, it seems that (1) our decoding\\nleverages the brain responses related to both the onset and the offset of the image and (2) category-\\nlevel information dominates these visual representations as early as 250 ms.\\n7 Table 1: Quantitative evaluation of reconstruction quality from MEG data on THINGS-MEG (com-\\npared to fMRI data on NSD (Allen et al., 2022) using a cross-validated Ridge regression). We re-\\nport PixCorr, SSIM, AlexNet(2), AlexNet(5), Inception, SwA V and CLIP (the side-arrow indicates\\nwhether better scores are higher or lower). In particular, this shows that fMRI betas as provided in\\nNSD are significantly easier to decode than MEG signals from THINGS-MEG.\\n',\n",
       " 'Low-level High-level\\nDataset PixCorr ↑SSIM↑AlexNet(2) ↑AlexNet(5) ↑Inception ↑CLIP↑SwA V ↓\\nNSD (fMRI) 0.305 0.366 0.962 0.977 0.910 0.917 0.410\\nTHINGS-MEG\\n(per-trial average)0.079 0.329 0.718 0.823 0.674 0.765 0.595\\nTHINGS-MEG\\n(per-subject average)0.088 0.333 0.747 0.855 0.712 0.804 0.576\\nTHINGS-MEG\\n(no average)0.069 0.308 0.668 0.733 0.613 0.668 0.636\\nGenerating images from MEG.',\n",
       " 'While framing decoding as a retrieval task yields promising\\nresults, it requires the true image to be in the retrieval set – a well-posed problem which presents\\nlimited use-cases in practice. To address this issue, we trained three distinct brain modules to predict\\nthe three embeddings that we use (see Section 2.5) to generate images (Fig. 5).',\n",
       " 'As confirmed by the\\nevaluation metrics of Table 1, the generated images look relatively good, with multiple generated\\nimages sharing the correct ground-truth category. However, they appear to contain limited low-level\\ninformation about the true image.\\n',\n",
       " 'Figure 5: Examples of generated images conditioned on MEG-based latent predictions. The groups\\nof three stacked rows represent best, average and worst generations, as evaluated by the sum of\\n(minus) SwA V and SSIM.\\n The application of a very similar pipeline on an analogous fMRI dataset (Allen et al., 2022; Ozcelik\\n& VanRullen, 2023) – using a simple Ridge regression – shows image reconstructions that share\\nboth high-level and low-level features with the true image Fig. S3). Together, these results suggest\\nthat it is not the reconstruction pipeline which fails to reconstruct low-level features, but rather the\\nMEG signals which contain little information at that level.\\n 8 4 D ISCUSSION\\nRelated work. The present study shares several elements with previous MEG and electroen-\\ncephalography (EEG) studies designed not to maximize decoding performance but to understand\\nthe cascade of visual processes in the brain. In particular, previous studies have trained linear mod-\\nels to either (1) classify a small set of images from brain activity (Grootswagers et al., 2019; King\\n& Wyart, 2021), (2) predict brain activity from the latent representations of the images (Cichy et al.,\\n2017) or (3) quantify the similarity between these two modalities with representational similarity\\nanalysis (RSA) (Cichy et al., 2017; Bankson et al., 2018; Grootswagers et al., 2019; Gifford et al.,\\n2022). While these studies also make use of image embeddings, their linear decoders are limited to\\nclassifying a small set of object classes, or to distinguishing pairs of images.\\n',\n",
       " 'In addition, several deep neural networks have been introduced to maximize the classification of\\nspeech (D ´efossez et al., 2022), mental load (Jiao et al., 2018) and images (Palazzo et al., 2020;\\nMcCartney et al., 2022; Bagchi & Bathula, 2022) from EEG recordings. In particular, Palazzo et al.\\n(2020) introduced a deep convolutional neural network to classify natural images from EEG signals.\\n However, the experimental protocol consisted of presenting all of the images of the same class within\\na single continuous block, which risks allowing the decoder to rely on autocorrelated noise, rather\\nthan informative brain activity patterns (Li et al., 2020). In any case, these EEG studies focus on the\\ncategorization of a relatively small number of images classes.\\n In sum, there is, to our knowledge, no MEG decoding study that learns end-to-end to reliably gen-\\nerate an open set of images.\\nImpact. The present work has both fundamental and practical impacts. First, the ability to decode\\ncomplex perceptual representations as a function of time promises to greatly facilitate our under-\\nstanding of the processes at stake during visual processing in the brain. There is considerable work\\ninspecting the nature and the timing of the representations built along the visual system. However,\\nthese results can be challenging to interpret, especially for high-level features. Generative decoding,\\non the contrary, provides concrete and, thus, interpretable predictions.',\n",
       " 'Second, the most obvious\\nuse-case of brain decoding technology is to assist patients whose brain lesions challenge communi-\\ncation. This use-case, however, requires real-time decoding, and thus limit the use of neuroimaging\\nmodalities with low temporal resolution such as fMRI. The present effort thus paves the way to\\nachieve this long-awaited goal.\\nLimitations. Our analyses highlight three main limitations to the decoding of images from MEG\\nsignals. First, the decoding of high-level semantic features prevails over low-level features: in\\nparticular, the generated images preserve semantics ( e.g. object categories) much better than low-\\nlevel features ( e.g. contours, shading). This phenomenon is difficult to attribute to our pipeline:\\nindeed, applying a similar procedure to 7T fMRI recordings achieves reasonably high reconstruction\\nof low-level features (Fig. S3). Rather, this result resonates with the fact that the spatial resolution\\nof MEG ( ≈cm) is much lower than 7T fMRI’s ( ≈mm). Second, the present approach directly\\ndepends on the pretraining of several models, and only learns end-to-end to align the MEG signals\\nto these pretrained embeddings. Our results show that this approach leads to better performance than\\nclassical computer vision features such as color histograms, fast-Fourier transforms and histogram\\nof oriented gradients (HOG). This is consistent with a recent MEG study by D ´efossez et al. (2022)\\nwhich showed, in the context of speech decoding, that pretrained embeddings outperformed a fully\\nend-to-end approach. Nevertheless, it remains to be tested whether (1) fine-tuning the image and\\ngeneration modules and (2) combining the different types of visual features could improve decoding\\nperformance.\\n',\n",
       " 'While the decoding of brain activity promises to help a variety of brain-\\nlesioned patients (Metzger et al., 2023; Moses et al., 2021; D ´efossez et al., 2022; Liu et al., 2023;\\nWillett et al., 2023), the rapid advances of this technology raise several ethical considerations, and\\nmost notably, the necessity to preserve mental privacy.',\n",
       " 'Several empirical findings are relevant to this\\nissue. Firstly, the decoding performance obtained with non-invasive recordings is only high for per-\\nceptual tasks.',\n",
       " 'By contrast, decoding accuracy considerably diminishes when individuals are tasked\\nto imagine representations (Horikawa & Kamitani, 2017; Tang et al., 2023). Second, decoding per-\\nformance seems to be severely compromised when participants are engaged in disruptive tasks, such\\n9 as counting backward (Tang et al., 2023). In other words, the subjects’ consent is not only a legal but\\nalso and primarily a technical requirement for brain decoding. To delve into these issues effectively,\\nwe endorse the open and peer-reviewed research standards.\\nConclusion. Overall, these results provide an important step towards the decoding of the visual\\nprocesses continuously unfolding in the human brain.\\n',\n",
       " 'REFERENCES\\nEmily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle,\\nMatthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al.',\n",
       " 'A massive 7T fMRI dataset to\\nbridge cognitive neuroscience and artificial intelligence. Nature neuroscience , 25(1):116–126,\\n2022.\\n Subhranil Bagchi and Deepti R Bathula. EEG-ConvTransformer for single-trial EEG-based visual\\nstimulus classification. Pattern Recognition , 129:108757, 2022.\\n Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,\\nAlexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al.',\n",
       " 'Vector-based\\nnavigation using grid-like representations in artificial agents.',\n",
       " 'Nature , 557(7705):429–433, 2018.\\n B.B. Bankson, M.N. Hebart, I.I.A. Groen, and C.I. Baker. The temporal evolution of conceptual\\nobject representations revealed through models of behavior, semantics and deep neural networks.\\n NeuroImage , 178:172–182, 2018. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.\\n 2018.05.037. URL https://www.sciencedirect.com/science/article/pii/\\nS1053811918304440 .\\n G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools , 2000.\\n Charlotte Caucheteux, Alexandre Gramfort, and Jean-R ´emi King.',\n",
       " 'Evidence of a predictive coding\\nhierarchy in the human brain listening to speech.',\n",
       " 'Nature human behaviour , 7(3):430–441, 2023.\\n Radoslaw Martin Cichy, Aditya Khosla, Dimitrios Pantazis, and Aude Oliva. Dynamics of scene\\nrepresentations in the human brain revealed by magnetoencephalography and deep neural net-\\nworks. NeuroImage , 153:346–358, 2017.\\n Alexandre D ´efossez, Charlotte Caucheteux, J ´er´emy Rapin, Ori Kabeli, and Jean-R ´emi King. De-\\ncoding speech from non-invasive brain recordings. arXiv preprint arXiv:2208.12266 , 2022.\\n Matteo Ferrante, Tommaso Boccato, and Nicola Toschi. Semantic brain decoding: from fMRI to\\nconceptually similar image reconstruction of visual stimuli. arXiv preprint arXiv:2212.06726 ,\\n2022.\\n Alessandro T Gifford, Kshitij Dwivedi, Gemma Roig, and Radoslaw M Cichy. A large and rich\\nEEG dataset for modeling human visual object recognition.',\n",
       " 'NeuroImage , 264:119754, 2022.\\n Tijl Grootswagers, Amanda K Robinson, and Thomas A Carlson.',\n",
       " 'The representational dynamics of\\nvisual objects in rapid serial visual processing streams.',\n",
       " 'NeuroImage , 188:668–679, 2019.\\n S´ebastien B Hausmann, Alessandro Marin Vargas, Alexander Mathis, and Mackenzie W Mathis.\\n',\n",
       " 'Measuring and modeling the motor system with machine learning. Current opinion in neurobiol-\\nogy, 70:11–23, 2021.\\n Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wick-\\nlin, and Chris I Baker.',\n",
       " 'A database of 1,854 object concepts and more than 26,000\\nnaturalistic object images.',\n",
       " 'PloS one , 14(10):e0223792, 2019.\\n Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis\\nKidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker. THINGS-data, a multi-\\nmodal collection of large-scale datasets for investigating object representations in human brain\\nand behavior. eLife , 12:e82580, feb 2023. ISSN 2050-084X. doi: 10.7554/eLife.82580. URL\\nhttps://doi.org/10.7554/eLife.82580 .\\n 10 Tomoyasu Horikawa and Yukiyasu Kamitani.',\n",
       " 'Generic decoding of seen and imagined objects using\\nhierarchical visual features.',\n",
       " 'David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional archi-\\ntecture in the cat’s visual cortex. The Journal of physiology , 160(1):106, 1962.\\n Vinay Jayaram and Alexandre Barachant. MOABB: trustworthy algorithm benchmarking for bcis.\\n Journal of neural engineering , 15(6):066011, 2018.\\n Zhicheng Jiao, Xinbo Gao, Ying Wang, Jie Li, and Haojun Xu. Deep convolutional neural networks\\nfor mental load classification based on EEG data. Pattern Recognition , 76:582–595, 2018.\\n Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human\\nbrain. Nature neuroscience , 8(5):679–685, 2005.\\n Nancy Kanwisher, Josh McDermott, and Marvin M Chun. The fusiform face area: a module in\\nhuman extrastriate cortex specialized for face perception. Journal of neuroscience , 17(11):4302–\\n4311, 1997.\\n',\n",
       " 'Jean-R ´emi King and Valentin Wyart. The human brain encodes a chronicle of visual events at each\\ninstant of time through the multiplexing of traveling waves.',\n",
       " 'Journal of Neuroscience , 41(34):\\n7224–7233, 2021.\\n Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980 , 2014.\\n',\n",
       " 'Ren Li, Jared S Johansen, Hamad Ahmed, Thomas V Ilyevsky, Ronnie B Wilbur, Hari M Bharad-\\nwaj, and Jeffrey Mark Siskind. The perils and pitfalls of block design for EEG classification\\nexperiments. IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(1):316–333,\\n2020.\\n Yan Liu, Zehao Zhao, Minpeng Xu, Haiqing Yu, Yanming Zhu, Jie Zhang, Linghao Bu, Xiaoluo\\nZhang, Junfeng Lu, Yuanning Li, et al.',\n",
       " 'Decoding and synthesizing tonal language speech from\\nbrain activity.',\n",
       " 'Science Advances , 9(23):eadh0478, 2023.\\n Weijian Mai and Zhijun Zhang.',\n",
       " 'Unify image reconstruction and captioning all in one\\ndiffusion model from human brain activity. arXiv preprint arXiv:2308.07428 , 2023.\\n Ben McCartney, Barry Devereux, and Jesus Martinez-del Rincon. A zero-shot deep metric learn-\\ning approach to brain–computer interfaces for image retrieval. Knowledge-Based Systems , 246:\\n108556, 2022.\\n Johannes Mehrer, Courtney J Spoerer, Emer C Jones, Nikolaus Kriegeskorte, and Tim C Kietzmann.\\n',\n",
       " 'An ecologically motivated image dataset for deep learning yields better models of human vision.\\n',\n",
       " 'Proceedings of the National Academy of Sciences , 118(8):e2011417118, 2021.\\n Sean L Metzger, Kaylo T Littlejohn, Alexander B Silva, David A Moses, Margaret P Seaton, Ran\\nWang, Maximilian E Dougherty, Jessie R Liu, Peter Wu, Michael A Berger, et al. A high-\\nperformance neuroprosthesis for speech decoding and avatar control. Nature , pp. 1–10, 2023.\\n',\n",
       " 'David A Moses, Sean L Metzger, Jessie R Liu, Gopala K Anumanchipalli, Joseph G Makin,\\nPengfei F Sun, Josh Chartier, Maximilian E Dougherty, Patricia M Liu, Gary M Abrams, et al.\\nNeuroprosthesis for decoding speech in a paralyzed person with anarthria. New England Journal\\nof Medicine , 385(3):217–227, 2021.\\n Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant.',\n",
       " 'Re-\\nconstructing visual experiences from brain activity evoked by natural movies.',\n",
       " 'Current biology ,\\n21(19):1641–1646, 2011.\\n John O’Keefe and Lynn Nadel. The hippocampus as a cognitive map.',\n",
       " 'Behavioral and Brain Sci-\\nences , 2(4):487–494, 1979.\\n Furkan Ozcelik and Rufin VanRullen. Brain-diffuser: Natural scene reconstruction from fMRI\\nsignals using generative latent diffusion. arXiv preprint arXiv:2303.05334 , 2023.\\n 11 Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and\\nMubarak Shah.',\n",
       " 'Decoding brain representations by multimodal learning of neural activity and\\nvisual features. IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(11):3833–\\n3849, 2020.\\n Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natural language supervision, 2021.\\n Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Joce-\\nlyn Faubert. Deep learning-based electroencephalography analysis: a systematic review. Journal\\nof neural engineering , 16(5):051001, 2019.\\n Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\\nJoshua Tenenbaum, and Evelina Fedorenko. Artificial neural networks accurately predict lan-\\nguage processing in the brain.',\n",
       " 'Paul S Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen,\\nAidan J Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, et al. Reconstructing\\nthe mind’s eye: fMRI-to-image with contrastive learning and diffusion priors. arXiv preprint\\narXiv:2305.18274 , 2023.\\n',\n",
       " 'Katja Seeliger, Umut G ¨uc ¸l¨u, Luca Ambrogioni, Yagmur G ¨uc ¸l¨ut¨urk, and Marcel AJ van Gerven.\\n',\n",
       " 'Generative adversarial networks for reconstructing natural images from brain activity.',\n",
       " 'NeuroIm-\\nage, 181:775–785, 2018.\\n Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models\\nfrom human brain activity.',\n",
       " 'bioRxiv , 2023. doi: 10.1101/2022.11.18.517004. URL https:\\n//www.biorxiv.org/content/early/2023/03/11/2022.11.18.517004 .\\n Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic reconstruction of con-\\ntinuous language from non-invasive brain recordings. Nature Neuroscience , pp. 1–9, 2023.\\n Armin Thomas, Christopher R ´e, and Russell Poldrack. Self-supervised learning of brain dynamics\\nfrom broad neuroimaging data. Advances in Neural Information Processing Systems , 35:21255–\\n21269, 2022.\\n Stefan Van der Walt, Johannes L Sch ¨onberger, Juan Nunez-Iglesias, Franc ¸ois Boulogne, Joshua D\\nWarner, Neil Yager, Emmanuelle Gouillart, and Tony Yu. scikit-image: image processing in\\npython. PeerJ , 2:e453, 2014.\\n',\n",
       " 'Rufin VanRullen and Leila Reddy. Reconstructing faces from fMRI patterns using deep generative\\nneural networks.',\n",
       " 'Communications biology , 2(1):193, 2019.\\n Francis R Willett, Erin M Kunz, Chaofei Fan, Donald T Avansino, Guy H Wilson, Eun Young\\nChoi, Foram Kamdar, Matthew F Glasser, Leigh R Hochberg, Shaul Druckmann, et al.',\n",
       " 'A high-\\nperformance speech neuroprosthesis. Nature , pp. 1–6, 2023.\\n Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J\\nDiCarlo.',\n",
       " 'Performance-optimized hierarchical models predict neural responses in higher visual\\ncortex.',\n",
       " 'Proceedings of the national academy of sciences , 111(23):8619–8624, 2014.\\n Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang\\nLiu, and Baochang Zhang. Controllable mind visual diffusion model.',\n",
       " 'A A PPENDIX\\nA.1 L INEAR RIDGE REGRESSION SCORES ON PRETRAINED IMAGE REPRESENTATIONS\\nWe provide a (5-fold cross-validated) Ridge regression baseline (Table S1) for comparison with our\\nbrain module results of Section 3, showing considerable improvements for the latter.\\n Table S1:',\n",
       " 'Image retrieval performance of a linear Ridge regression baseline on pretrained image\\nrepresentations\\nTop-5 acc (%) ↑ Median relative rank ↓\\nLatent kind Latent name Small set Large set Small set Large set\\nText/Image\\nalignmentCLIP-Vision (CLS) 10.5 0.50 0.23 0.34\\nCLIP-Text (mean) 6.0 0.25 0.42 0.43\\nCLIP-Vision (mean) 5.5 0.46 0.32 0.37\\nFeature\\nengineeringColor histogram 7.0 0.33 0.31 0.40\\nLocal binary patterns (LBP) 3.5 0.37 0.34 0.44\\nFFT 2D (as real) 4.5 0.46 0.40 0.45\\nHOG 3.0 0.42 0.45 0.46\\nFFT 2D (log-PSD and angle) 2.0 0.37 0.47 0.46\\nVariational\\nautoencoderAutoKL 7.5 0.54 0.24 0.38\\nVDV AE 8.0 0.50 0.33 0.43\\nSelf-supervised\\nlearning DINOv2 (CLS) 7.5 0.46 0.25 0.35\\nSupervisedVGG-19 12.5 1.04 0.18 0.33\\nResNet-101 4.0 0.37 0.36 0.42\\nDenseNet-201 5.0 0.29 0.39 0.45\\nWide ResNet-101-2 3.5 0.42 0.40 0.46\\nMobileNet v3 3.5 0.42 0.40 0.42\\nA.2 H YPERPARAMETER SEARCH\\n We run a hyperparameter search to find an appropriate configuration (MEG preprocessing, opti-\\nmizer, brain module architecture and loss definition) for the MEG-to-image retrieval task ( λ= 0).\\n We randomly split the 79,392 (MEG, image) pairs of the adapted training set (Section 2.8) into\\n60%-20%-20% train, valid and test splits such that all presentations of a given image are contained\\nin the same split. We use the validation split to perform early stopping and the test split to evaluate\\nthe performance of a configuration.\\n For the purpose of this search we pick CLIP-Vision (CLS) latent as a representative latent, since\\nit achieved good retrieval performance in preliminary experiments. We run the search six times\\nusing two different random seed initializations for the brain module and three different random\\ntrain/valid/test splits.',\n",
       " 'Fig. S1 summarizes the results of this hyperparameter search.\\n Based on this search, we use the following configuration: MEG window (tmin, tmax)of\\n[−0.5,1.0]s, learning rate of 3×10−4, batch size of 128, brain module with two convolutional\\nblocks and both the spatial attention and subject layers of D ´efossez et al. (2022), affine projection\\ntemporal aggregation layer with a single block in the CLIP projection head, and full CLIP loss (in-\\ncluding learned temperature parameter, normalization along both axes and symmetric terms). The\\nfinal architecture configuration is presented in Table S2.\\n A.3 F ULL-WINDOW MEG- BASED IMAGE RETRIEVALS\\nFig. S2 shows examples of retrieved images based on the best performing latents identified in Sec-\\ntion 3.\\n13 Figure S1: Hyperparameter search results for the MEG-to-image retrieval task, presenting the impact\\nof (A) optimizer learning rate and batch size, ( B) number of convolutional blocks and use of spatial\\nattention and/or subject-specific layers in the brain module, ( C) MEG window parameters, ( D) type\\nof temporal aggregation layer and number of blocks in the CLIP projection head of the brain module,\\nand ( E) CLIP loss configuration (normalization axes, use of learned temperature parameter and use\\nof symmetric terms). Chance-level performance top-5 accuracy is 0.05%.\\n Table S2: Brain module configuration adapted from D ´efossez et al. (2022) for use with a target latent\\nof size 768 ( e.g. CLIP-Vision (CLS), see Section 2.4) in retrieval settings.\\n Layer Input shape Output shape # parameters\\nSpatial attention block (272, 181) (270, 181) 552,960\\nLinear projection (270, 181) (270, 181) 73,170\\nSubject-specific linear layer (270, 181) (270, 181) 291,600\\nResidual dilated conv block 1 (270, 181) (320, 181) 1,183,360\\nResidual dilated conv block 2 (320, 181) (320, 181) 1,231,360\\nLinear projection (320, 181) (2048, 181) 1,518,208\\nTemporal aggregation (2048, 181) (2048, 1) 182\\nMLP projector (2048, 1) (768, 1) 1,573,632\\nTotal 6,424,472\\n14 Figure S2: Representative examples of retrievals (top-4) using models trained on full windows (from\\n-0.5 s to 1 s after image onset). Retrieval set: N= 6,059images from 1,196categories.\\n',\n",
       " 'A.4 I MAGE EMBEDDINGS\\nWe evaluate the performance of linear baselines and of a deep convolutional neural network on the\\nMEG-to-image retrieval task using a set of classic visual embeddings. We grouped these embeddings\\nby their corresponding paradigm:\\nSupervised learning. DenseNet-121,DenseNet-169,DenseNet-201, MobileNet v2, MobileNet\\nv3, ResNet-101, ResNet-18, ResNet-50, ResNext101-32-8d, ResNext50-32-4d, VGG-16,VGG-19,\\nWide ResNet-101-2, Wide ResNet-50-2.\\nText/Image alignment. CLIP-Vision, CLIP-Text, and their CLS and MEAN pooling.\\n Self-supervised learning. DINOv1, DINOv2 and their CLS and MEAN pooling.\\n 15 Figure S3:',\n",
       " 'Examples of generated images conditioned on fMRI-based latent predictions. The groups\\nof three stacked rows represent best, average and worst retrievals, as evaluated by the sum of (minus)\\nSwA V and SSIM.\\n',\n",
       " 'The activations of the 31 first layers of the very deep variational-\\nautoencoder (VDV AE), and the Kullback-Leibler variational-autoencoder (AutoKL) used in the gen-\\nerative module (Section 2.5).\\n',\n",
       " 'The color histogram of the seen image (8 bins per channels); the local binary\\npatterns (LBP) using the implementation in OpenCV 2 (Bradski, 2000) with ’uniform’ method,\\nP= 8 andR= 1; the Histogram of Oriented Gradients (HOG) using the implementation of sk-\\nimage (Van der Walt et al., 2014) with 8 orientations, 8 pixels-per-cell and 2 cells-per-block.\\n A.5 7T FMRI DATASET\\nThe Natural Scenes Dataset (NSD) (Allen et al., 2022) contains fMRI data from 8 participants\\nviewing a total of 73,000 RGB images. It has been successfully used for reconstructing seen images\\nfrom fMRI in several studies (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023; Scotti et al.,\\n2023). In particular, these studies use a highly preprocessed, compact version of fMRI data (“betas”)\\nobtained through generalized linear models fitted across multiple repetitions of the same image.\\n Each participant saw a total of 10,000 unique images (repeated 3 times each) across 37 sessions.\\n Each session consisted in 12 runs of 5 minutes each, where each image was seen during 3 s, with\\na 1-s blank interval between two successive image presentations. Among the 8 participants, only 4\\n(namely 1, 2, 5 and 7) completed all sessions.\\n To compute the three latents used to reconstruct the seen images from fMRI data (as described in\\nSection 2.5) we follow Ozcelik & VanRullen (2023) and train and evaluate three distinct Ridge\\nregression models using the exact same split. That is, for each of the four remaining participants,\\nthe 9,000 uniquely-seen-per-participant images (and their three repetitions) are used for training,\\nand a common set of 1000 images seen by all participant is kept for evaluation (also with their three\\nrepetitions). We report reconstructions and metrics for participant 1.\\nTheαcoefficient for the L2-regularization of the regressions are cross-validated with a 5-fold\\nscheme on the training set of each subject. We follow the same standardization scheme for inputs\\nand predictions as in (Ozcelik & VanRullen, 2023).\\n',\n",
       " 'Fig. S3 presents generated images obtained using the NSD dataset (Allen et al., 2022).\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "sents, vecs = process(text)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < 60:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > 3000:\n",
    "        threshold = 0.6\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < 60 or div_len > 3000:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)\n",
    "\n",
    "final_texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
